{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "736a259e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLearning Notes\\n\\nI messed up a whole bunch of things in this code vs https://github.com/openai/spinningup/blob/master/spinup/examples/pytorch/pg_math/1_simple_pg.py:\\n1. Reward normalization\\n2. Didn\\'t use \"Categorical\" distribution for the policy\\n3. Loss is something negative and huge...\\n4. Sampled from softmax using argmax - should have used torch.distributions.Categorical or jax.random.categorical -- which uses gumbel softmax trick to make it differentiable\\n5. Model was huge - i used 512x512x2 vs 32x2\\n6. LR was too small - 1e-4 vs 1e-2\\n7. Batch size was too small - 32 vs 5000 -- also samples are each time step, not each episode\\n8. INCORRECT OBSERVATIONS USED IN THE EXPERIENCE BUFFER - I used the newly sampled ones after running env.step\\n   but this was incorrect, since I needed use the previous ones...\\n \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Learning Notes\n",
    "\n",
    "I messed up a whole bunch of things in this code vs https://github.com/openai/spinningup/blob/master/spinup/examples/pytorch/pg_math/1_simple_pg.py:\n",
    "1. Reward normalization\n",
    "2. Didn't use \"Categorical\" distribution for the policy\n",
    "3. Loss is something negative and huge...\n",
    "4. Sampled from softmax using argmax - should have used torch.distributions.Categorical or jax.random.categorical -- which uses gumbel softmax trick to make it differentiable\n",
    "5. Model was huge - i used 512x512x2 vs 32x2\n",
    "6. LR was too small - 1e-4 vs 1e-2\n",
    "7. Batch size was too small - 32 vs 5000 -- also samples are each time step, not each episode\n",
    "8. INCORRECT OBSERVATIONS USED IN THE EXPERIENCE BUFFER - I used the newly sampled ones after running env.step\n",
    "   but this was incorrect, since I needed use the previous ones...\n",
    " \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0be9fb82-9b9c-4eb8-bd12-44aee0215ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import optax\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from gymnasium import wrappers\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "# from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ea56f6a-916d-420b-9456-8d1544e8b91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anujkhare/miniconda3/envs/foo1/lib/python3.12/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/anujkhare/code/save_videos2 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# The environment is described in the following link:\n",
    "# https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "\n",
    "# Record videos\n",
    "trigger = lambda _: True\n",
    "env = wrappers.RecordVideo(env, video_folder=\"./save_videos2\", episode_trigger=trigger, disable_logger=True, video_length=1000)\n",
    "env = wrappers.RecordEpisodeStatistics(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e4d2bbf-71f7-491d-985e-533b527bef47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04455611 -0.02220803 -0.01992462 -0.0191059 ]\n",
      "0 ---> [-0.04500027 -0.21703866 -0.02030674  0.26722458] 1.0 False False {}\n",
      "1 ---> [-0.04934104 -0.02163287 -0.01496225 -0.03179345] 1.0 False False {}\n",
      "1 ---> [-0.0497737   0.17370042 -0.01559812 -0.32915932] 1.0 False False {}\n",
      "1 ---> [-0.04629969  0.3690409  -0.02218131 -0.6267201 ] 1.0 False False {}\n",
      "1 ---> [-0.03891887  0.56446534 -0.03471571 -0.92630535] 1.0 False False {}\n",
      "1 ---> [-0.02762957  0.76003844 -0.05324182 -1.2296927 ] 1.0 False False {}\n",
      "1 ---> [-0.0124288   0.95580345 -0.07783566 -1.5385698 ] 1.0 False False {}\n",
      "1 ---> [ 0.00668727  1.151771   -0.10860706 -1.8544916 ] 1.0 False False {}\n",
      "0 ---> [ 0.02972269  0.9579972  -0.1456969  -1.5974113 ] 1.0 False False {}\n",
      "1 ---> [ 0.04888264  1.1545147  -0.17764512 -1.9317479 ] 1.0 False False {}\n",
      "0 ---> [ 0.07197293  0.96168447 -0.21628007 -1.6990079 ] 1.0 True False {'episode': {'r': 11.0, 'l': 11, 't': 0.020849}}\n"
     ]
    }
   ],
   "source": [
    "#@title Visualize the environment!\n",
    "episode_over = False\n",
    "obs, _ = env.reset()\n",
    "print(obs)\n",
    "while not episode_over:\n",
    "    # action, _ = model.predict(obs, deterministic=True)\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, termination, truncation, info = env.step(action)\n",
    "    print(action, '--->', obs, reward, termination, truncation, info)\n",
    "\n",
    "    episode_over = termination or truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f0b4f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Params:\n",
    "    learning_rate: float = 1e-2\n",
    "    batch_size: int = 4096  # Use a large batch size for policy iteration!\n",
    "    max_steps: int = 100\n",
    "    obs_dim: int = 4\n",
    "\n",
    "    # model related\n",
    "    hidden_dims: list[int] = dataclasses.field(default_factory=lambda: [32])\n",
    "\n",
    "    # initial exploration rate\n",
    "    epsilon: int = 0.15\n",
    "    # epsilon is decayed over time with a cosine schedule, starting at `epsilon`\n",
    "    # and ending at `epsilon_min` over `max_steps` steps.\n",
    "    epsilon_min: float = 0.1  # always explore at least 10% of the time\n",
    "\n",
    "    def get_epsilon(self, step: int) -> float:\n",
    "        if step > self.max_steps:\n",
    "            raise ValueError(\"step must be less than max_steps\")\n",
    "        return self.epsilon_min + (self.epsilon - self.epsilon_min) * (1 + np.cos(np.pi * step / self.max_steps) ) / 2 \n",
    "\n",
    "\n",
    "params = Params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9c21827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define the policy\n",
    "import equinox as eqx\n",
    "import jax\n",
    "\n",
    "class MLP(eqx.Module):\n",
    "    layers: list\n",
    "    output_proj: eqx.nn.Linear\n",
    "\n",
    "    def __init__(\n",
    "        self, in_size: int, hidden_dims: list[int], out_size: int, key: jax.random.PRNGKey):\n",
    "        self.layers = []\n",
    "        prev_dim = in_size\n",
    "        for dim in hidden_dims:\n",
    "            key, layer_key = jax.random.split(key)\n",
    "            self.layers.append(\n",
    "                eqx.nn.Linear(\n",
    "                    in_features=prev_dim,\n",
    "                    out_features=dim,\n",
    "                    use_bias=True,\n",
    "                    key=layer_key\n",
    "                )\n",
    "            )\n",
    "            prev_dim = dim\n",
    "        self.output_proj = eqx.nn.Linear(\n",
    "            in_features=prev_dim,\n",
    "            out_features=out_size,\n",
    "            use_bias=True,\n",
    "            key=key\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            x = jax.nn.relu(x)\n",
    "        # log probs (log(softmax(x))\n",
    "        x = self.output_proj(x)\n",
    "        return x\n",
    "        # return x - jax.scipy.special.logsumexp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69566410",
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def policy(model: MLP, obs: np.ndarray, epsilon: float, key: jax.random.PRNGKey):\n",
    "    key1, key2, key3 = jax.random.split(key, 3)\n",
    "\n",
    "    # Find best action based on policy model\n",
    "    logits = jax.vmap(model)(obs)\n",
    "    # unnormalized log-probabilities are represented by the model output logits -- treat as categorical distribution\n",
    "    model_actions = tfp.distributions.Categorical(logits=logits).sample(seed=key3)\n",
    "\n",
    "    # # maybe explore based on epsilon-greedy strategy\n",
    "    # p_explore = jax.random.uniform(key1, (len(obs),))\n",
    "    # random_actions = jax.random.randint(key2, (len(obs),), minval=0, maxval=2)\n",
    "\n",
    "    # final_actions = jax.numpy.where(p_explore < epsilon, random_actions, model_actions)\n",
    "    final_actions = model_actions\n",
    "    return final_actions, model_actions\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class SampleOutput:\n",
    "    observations: np.ndarray = dataclasses.field(default_factory=lambda: np.array([[]]))\n",
    "    actions: np.ndarray = dataclasses.field(default_factory=lambda: np.array([]))\n",
    "    rewards: np.ndarray = dataclasses.field(default_factory=lambda: np.array([]))\n",
    "    rewards_to_go: np.ndarray = dataclasses.field(default_factory=lambda: np.array([]))\n",
    "    valid: np.ndarray = dataclasses.field(default_factory=lambda: np.array([]))\n",
    "    avg_expected_reward: float = 0.0\n",
    "    avg_action: float = 0.0\n",
    "    avg_episode_length: float = 0.0\n",
    "    num_episodes: int = 0\n",
    "\n",
    "    def num_samples(self):\n",
    "        return len(self.observations)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        def _weighted_sum(attr):\n",
    "            return (getattr(self, attr) * self.num_samples() + getattr(other, attr) * other.num_samples()) / (self.num_samples() + other.num_samples())\n",
    "        return SampleOutput(\n",
    "            observations=np.concatenate([self.observations, other.observations]),\n",
    "            actions=np.concatenate([self.actions, other.actions]),\n",
    "            rewards=np.concatenate([self.rewards, other.rewards]),\n",
    "            rewards_to_go=np.concatenate([self.rewards_to_go, other.rewards_to_go]),\n",
    "            valid=np.concatenate([self.valid, other.valid]),\n",
    "            num_episodes=self.num_episodes + other.num_episodes,\n",
    "            avg_expected_reward=_weighted_sum('avg_expected_reward'),\n",
    "            avg_action=_weighted_sum('avg_action'),\n",
    "            avg_episode_length=_weighted_sum('avg_episode_length')\n",
    "        )\n",
    "\n",
    "def sample_from_policy(envs: gym.vector.VectorEnv, model: MLP, epsilon: float, key: jax.random.PRNGKey):\n",
    "    \"\"\"Run all envs in parallel, return the trajectory of each env.\"\"\"\n",
    "    obs, _ = envs.reset()\n",
    "    episode_over = np.array([False] * envs.num_envs)\n",
    "\n",
    "    # Track the actions, rewards, and gradients for policy optimization\n",
    "    trajectory_observations = []\n",
    "    trajectory_rewards = []\n",
    "    trajectory_actions = []\n",
    "    trajectory_valid = []\n",
    "\n",
    "    # iterate through the vectorized environments\n",
    "    while np.any(~episode_over):\n",
    "        trajectory_observations.append(obs)\n",
    "\n",
    "        key_used, key = jax.random.split(key)\n",
    "        actions, _ = policy(model, obs, epsilon, key_used)\n",
    "        obs, rewards, terminated, truncated, _ = envs.step(np.array(actions))\n",
    "        episode_over = episode_over | terminated | truncated\n",
    "\n",
    "        # Mask out everything for episodes that are over\n",
    "        mask = ~episode_over\n",
    "        trajectory_rewards.append(rewards)\n",
    "        trajectory_actions.append(actions)\n",
    "        trajectory_valid.append(mask)\n",
    "\n",
    "    trajectory_observations = np.einsum('Tbd->bTd', np.stack(trajectory_observations))\n",
    "    trajectory_actions = np.stack(trajectory_actions).T\n",
    "    trajectory_rewards = np.stack(trajectory_rewards).T\n",
    "    trajectory_valid = np.stack(trajectory_valid).T\n",
    "\n",
    "    # compute the rewards to go\n",
    "    rewards_to_go = np.where(trajectory_valid, trajectory_rewards, 0)[:, ::-1].cumsum(axis=-1)[:, ::-1]\n",
    "\n",
    "    avg_expected_reward = np.mean(rewards_to_go[:, 0])  # average expected rewards for trajectories\n",
    "    avg_action = np.mean(trajectory_actions)\n",
    "    avg_episode_length = np.sum(trajectory_valid, axis=-1).mean()\n",
    "    num_episodes = len(trajectory_observations)\n",
    "\n",
    "    # Flatten all the (state, action, reward) tuples across episodes \n",
    "    # it doesn't matter which episode they came from (as long as we keep a track of the rewards!)\n",
    "    trajectory_valid = trajectory_valid.ravel()  # (T*B, )\n",
    "\n",
    "    return SampleOutput(\n",
    "        observations=trajectory_observations.reshape(-1, trajectory_observations.shape[-1])[trajectory_valid],\n",
    "        actions=trajectory_actions.ravel()[trajectory_valid],\n",
    "        rewards=trajectory_rewards.ravel()[trajectory_valid],\n",
    "        rewards_to_go=rewards_to_go.ravel()[trajectory_valid],\n",
    "        valid=trajectory_valid,\n",
    "        avg_expected_reward=avg_expected_reward,\n",
    "        avg_action=avg_action,\n",
    "        avg_episode_length=avg_episode_length,\n",
    "        num_episodes=num_episodes\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "034071f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, obs, actions, weights):\n",
    "    logits = jax.vmap(model)(obs)\n",
    "    log_probs = tfp.distributions.Categorical(logits=logits).log_prob(actions)\n",
    "\n",
    "    # the loss is technically the log likelihood of the actions weighted by the rewards-to-go\n",
    "    return -jax.numpy.sum(log_probs * weights) / len(obs)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def update_model(model, obs, actions, weights, optimizer, opt_state):\n",
    "    loss, grad = eqx.filter_value_and_grad(loss_fn)(model, obs, actions, weights)\n",
    "    updates, opt_state = optimizer.update(grad, opt_state, eqx.filter(model, eqx.is_array))\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "152a5081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class SampleAndUpdatePolicyOutputs:\n",
    "    model: MLP\n",
    "    opt_state: optax.OptState\n",
    "    loss: float\n",
    "    avg_expected_reward: float\n",
    "    avg_action: float\n",
    "    avg_episode_length: float\n",
    "    num_samples: int\n",
    "\n",
    "\n",
    "def sample_batch_and_update_policy(\n",
    "    envs: gym.vector.VectorEnv,\n",
    "    model: MLP, optimizer: optax.GradientTransformation,\n",
    "    opt_state: optax.OptState,\n",
    "    batch_size: int,\n",
    "    epsilon: float,\n",
    "    key: jax.random.PRNGKey):\n",
    "    # Sample at least \"batch_size\" time steps (state,action,reward)s from the policy\n",
    "    samples = None\n",
    "    while True:\n",
    "        key_used, key = jax.random.split(key)\n",
    "        new_samples = sample_from_policy(envs, model, epsilon, key_used)\n",
    "        if samples is None:\n",
    "            samples = new_samples\n",
    "        else:\n",
    "            samples = samples + new_samples\n",
    "        if samples.num_samples() >= batch_size:\n",
    "            break\n",
    "    # print(f\"Sampled {samples.num_samples()} samples from {samples.num_episodes} episodes\")\n",
    "\n",
    "    # Compute the loss and the gradient\n",
    "    model, opt_state, loss = update_model(\n",
    "        model,\n",
    "        samples.observations,\n",
    "        samples.actions,\n",
    "        samples.rewards_to_go,\n",
    "        optimizer,\n",
    "        opt_state)\n",
    "    return SampleAndUpdatePolicyOutputs(\n",
    "        model=model, opt_state=opt_state,\n",
    "        loss=loss,\n",
    "        avg_expected_reward=samples.avg_expected_reward,\n",
    "        avg_action=samples.avg_action,\n",
    "        avg_episode_length=samples.avg_episode_length,\n",
    "        num_samples=samples.num_samples(),\n",
    "    )\n",
    "\n",
    "\n",
    "def train(envs, model, optimizer, params, key: jax.random.PRNGKey):\n",
    "    opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "    for step in range(params.max_steps):\n",
    "        key_used, key = jax.random.split(key)\n",
    "        outputs = sample_batch_and_update_policy(\n",
    "            envs, model, optimizer, opt_state, params.batch_size, params.get_epsilon(step), key_used)\n",
    "        model, opt_state = outputs.model, outputs.opt_state\n",
    "        if step % 1 == 0:\n",
    "            print(f\"Step {step}: Loss {outputs.loss} Avg Reward: {outputs.avg_expected_reward} Avg Episode Len: {outputs.avg_episode_length}\")\n",
    "            print(f\"Batch size: {outputs.num_samples} Avg action: {outputs.avg_action}\")\n",
    "            # print(f\"New epsilon: {params.get_epsilon(step)}\")\n",
    "    return model, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f474ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Vectorize the enviroment such that we can sample multiple trajectories concurrently\n",
    "envs = gym.make_vec(\"CartPole-v1\", num_envs=32, vectorization_mode='vector_entry_point')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4c5dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "optim  = optax.adam(params.learning_rate)\n",
    "model = MLP(\n",
    "    in_size=4,\n",
    "    hidden_dims=params.hidden_dims + [2],\n",
    "    out_size=2,\n",
    "    key=jax.random.PRNGKey(12513)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0479186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 10:03:21.985288: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735639401.998689 1735472 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735639402.002472 1735472 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss 9.580428123474121 Avg Reward: 21.104222039123965 Avg Episode Len: 21.104222039123965\n",
      "Batch size: 4703 Avg action: 0.5674805225324212\n",
      "Step 1: Loss 9.169675827026367 Avg Reward: 20.72004785853342 Avg Episode Len: 20.72004785853342\n",
      "Batch size: 4623 Avg action: 0.5566305391264313\n",
      "Step 2: Loss 10.089892387390137 Avg Reward: 22.224314151392527 Avg Episode Len: 22.224314151392527\n",
      "Batch size: 4201 Avg action: 0.5353013828589164\n",
      "Step 3: Loss 10.432099342346191 Avg Reward: 23.31463500563698 Avg Episode Len: 23.31463500563698\n",
      "Batch size: 4435 Avg action: 0.5259911205168764\n",
      "Step 4: Loss 11.014314651489258 Avg Reward: 24.926968085106385 Avg Episode Len: 24.926968085106385\n",
      "Batch size: 4700 Avg action: 0.5293880168274832\n",
      "Step 5: Loss 12.52457046508789 Avg Reward: 25.885930696661827 Avg Episode Len: 25.885930696661827\n",
      "Batch size: 4134 Avg action: 0.5248507545547348\n",
      "Step 6: Loss 13.75375747680664 Avg Reward: 29.79821220004247 Avg Episode Len: 29.79821220004247\n",
      "Batch size: 4709 Avg action: 0.519423886956772\n",
      "Step 7: Loss 14.38609504699707 Avg Reward: 33.43423178613396 Avg Episode Len: 33.43423178613396\n",
      "Batch size: 4255 Avg action: 0.5230955467276548\n",
      "Step 8: Loss 14.703356742858887 Avg Reward: 33.15436557788945 Avg Episode Len: 33.15436557788945\n",
      "Batch size: 4179 Avg action: 0.5221200871912989\n",
      "Step 9: Loss 17.628698348999023 Avg Reward: 38.066035184422006 Avg Episode Len: 38.066035184422006\n",
      "Batch size: 4853 Avg action: 0.5159576862379074\n",
      "Step 10: Loss 15.085443496704102 Avg Reward: 34.51579865048764 Avg Episode Len: 34.51579865048764\n",
      "Batch size: 4409 Avg action: 0.5136307236988868\n",
      "Step 11: Loss 17.62702178955078 Avg Reward: 40.77088543670643 Avg Episode Len: 40.77088543670643\n",
      "Batch size: 5198 Avg action: 0.5057445534093309\n",
      "Step 12: Loss 17.779508590698242 Avg Reward: 42.69238335173937 Avg Episode Len: 42.69238335173937\n",
      "Batch size: 5433 Avg action: 0.503239863962233\n",
      "Step 13: Loss 19.524688720703125 Avg Reward: 45.3837728953857 Avg Episode Len: 45.3837728953857\n",
      "Batch size: 5678 Avg action: 0.5035394312419342\n",
      "Step 14: Loss 20.747148513793945 Avg Reward: 50.887732443382504 Avg Episode Len: 50.887732443382504\n",
      "Batch size: 4813 Avg action: 0.5011743395601435\n",
      "Step 15: Loss 19.90384292602539 Avg Reward: 50.687905009257356 Avg Episode Len: 50.687905009257356\n",
      "Batch size: 4861 Avg action: 0.5022689733667831\n",
      "Step 16: Loss 22.2654972076416 Avg Reward: 56.39432133295085 Avg Episode Len: 56.39432133295085\n",
      "Batch size: 5229 Avg action: 0.5046414599035631\n",
      "Step 17: Loss 23.65118408203125 Avg Reward: 61.60869289340101 Avg Episode Len: 61.60869289340101\n",
      "Batch size: 5910 Avg action: 0.5022538078682147\n",
      "Step 18: Loss 21.517698287963867 Avg Reward: 58.63487838560228 Avg Episode Len: 58.63487838560228\n",
      "Batch size: 5612 Avg action: 0.5050795357701012\n",
      "Step 19: Loss 24.768037796020508 Avg Reward: 70.91350681364946 Avg Episode Len: 70.91350681364946\n",
      "Batch size: 4513 Avg action: 0.49821411758062023\n",
      "Step 20: Loss 26.428436279296875 Avg Reward: 75.40186220225658 Avg Episode Len: 75.40186220225658\n",
      "Batch size: 4786 Avg action: 0.4985576739187103\n",
      "Step 21: Loss 30.49626350402832 Avg Reward: 81.98803762159069 Avg Episode Len: 81.98803762159069\n",
      "Batch size: 5243 Avg action: 0.4944674926289572\n",
      "Step 22: Loss 33.452789306640625 Avg Reward: 92.270983276884 Avg Episode Len: 92.270983276884\n",
      "Batch size: 5905 Avg action: 0.4910094444675794\n",
      "Step 23: Loss 29.11362075805664 Avg Reward: 86.72636261261262 Avg Episode Len: 86.72636261261262\n",
      "Batch size: 5550 Avg action: 0.4904842044425377\n",
      "Step 24: Loss 34.43870162963867 Avg Reward: 105.0343210244193 Avg Episode Len: 105.0343210244193\n",
      "Batch size: 6716 Avg action: 0.49386537569745076\n",
      "Step 25: Loss 37.756526947021484 Avg Reward: 117.08098961810656 Avg Episode Len: 117.08098961810656\n",
      "Batch size: 7489 Avg action: 0.49994626238560574\n",
      "Step 26: Loss 42.291229248046875 Avg Reward: 129.75 Avg Episode Len: 129.75\n",
      "Batch size: 4152 Avg action: 0.49843096234309625\n",
      "Step 27: Loss 44.40727996826172 Avg Reward: 146.75 Avg Episode Len: 146.75\n",
      "Batch size: 4696 Avg action: 0.49710181451612906\n",
      "Step 28: Loss 56.17879104614258 Avg Reward: 186.875 Avg Episode Len: 186.875\n",
      "Batch size: 5980 Avg action: 0.5020161290322581\n",
      "Step 29: Loss 70.53600311279297 Avg Reward: 235.34375 Avg Episode Len: 235.34375\n",
      "Batch size: 7531 Avg action: 0.49741331096196867\n",
      "Step 30: Loss 80.8420639038086 Avg Reward: 251.59375 Avg Episode Len: 251.59375\n",
      "Batch size: 8051 Avg action: 0.495375\n",
      "Step 31: Loss 87.61003112792969 Avg Reward: 289.15625 Avg Episode Len: 289.15625\n",
      "Batch size: 9253 Avg action: 0.4935\n",
      "Step 32: Loss 89.02617645263672 Avg Reward: 261.25 Avg Episode Len: 261.25\n",
      "Batch size: 8360 Avg action: 0.4984375\n",
      "Step 33: Loss 106.75988006591797 Avg Reward: 343.15625 Avg Episode Len: 343.15625\n",
      "Batch size: 10981 Avg action: 0.4985625\n",
      "Step 34: Loss 113.27153778076172 Avg Reward: 398.25 Avg Episode Len: 398.25\n",
      "Batch size: 12744 Avg action: 0.500125\n",
      "Step 35: Loss 103.92584228515625 Avg Reward: 352.5 Avg Episode Len: 352.5\n",
      "Batch size: 11280 Avg action: 0.4996875\n",
      "Step 36: Loss 104.54395294189453 Avg Reward: 361.6875 Avg Episode Len: 361.6875\n",
      "Batch size: 11574 Avg action: 0.50175\n",
      "Step 37: Loss 114.35997009277344 Avg Reward: 415.25 Avg Episode Len: 415.25\n",
      "Batch size: 13288 Avg action: 0.500125\n",
      "Step 38: Loss 109.69161224365234 Avg Reward: 389.15625 Avg Episode Len: 389.15625\n",
      "Batch size: 12453 Avg action: 0.501625\n",
      "Step 39: Loss 92.05803680419922 Avg Reward: 318.78125 Avg Episode Len: 318.78125\n",
      "Batch size: 10201 Avg action: 0.5063125\n",
      "Step 40: Loss 101.8580322265625 Avg Reward: 349.21875 Avg Episode Len: 349.21875\n",
      "Batch size: 11175 Avg action: 0.504375\n",
      "Step 41: Loss 115.06583404541016 Avg Reward: 418.75 Avg Episode Len: 418.75\n",
      "Batch size: 13400 Avg action: 0.50075\n",
      "Step 42: Loss 116.53640747070312 Avg Reward: 440.65625 Avg Episode Len: 440.65625\n",
      "Batch size: 14101 Avg action: 0.5005\n",
      "Step 43: Loss 115.70948791503906 Avg Reward: 448.3125 Avg Episode Len: 448.3125\n",
      "Batch size: 14346 Avg action: 0.4995\n",
      "Step 44: Loss 115.8250732421875 Avg Reward: 449.34375 Avg Episode Len: 449.34375\n",
      "Batch size: 14379 Avg action: 0.5\n",
      "Step 45: Loss 119.62641906738281 Avg Reward: 467.375 Avg Episode Len: 467.375\n",
      "Batch size: 14956 Avg action: 0.499875\n",
      "Step 46: Loss 116.75463104248047 Avg Reward: 464.40625 Avg Episode Len: 464.40625\n",
      "Batch size: 14861 Avg action: 0.5001875\n",
      "Step 47: Loss 117.5395278930664 Avg Reward: 473.375 Avg Episode Len: 473.375\n",
      "Batch size: 15148 Avg action: 0.4998125\n",
      "Step 48: Loss 120.58224487304688 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4995\n",
      "Step 49: Loss 120.1507797241211 Avg Reward: 485.6875 Avg Episode Len: 485.6875\n",
      "Batch size: 15542 Avg action: 0.500625\n",
      "Step 50: Loss 120.16209411621094 Avg Reward: 491.75 Avg Episode Len: 491.75\n",
      "Batch size: 15736 Avg action: 0.4995\n",
      "Step 51: Loss 121.21632385253906 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4998125\n",
      "Step 52: Loss 117.63160705566406 Avg Reward: 487.59375 Avg Episode Len: 487.59375\n",
      "Batch size: 15603 Avg action: 0.500375\n",
      "Step 53: Loss 119.6270980834961 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.500375\n",
      "Step 54: Loss 115.92536163330078 Avg Reward: 497.96875 Avg Episode Len: 497.96875\n",
      "Batch size: 15935 Avg action: 0.4995625\n",
      "Step 55: Loss 118.6274185180664 Avg Reward: 495.71875 Avg Episode Len: 495.71875\n",
      "Batch size: 15863 Avg action: 0.5000625\n",
      "Step 56: Loss 116.92936706542969 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.5008125\n",
      "Step 57: Loss 116.82743835449219 Avg Reward: 492.875 Avg Episode Len: 492.875\n",
      "Batch size: 15772 Avg action: 0.4996875\n",
      "Step 58: Loss 119.4937973022461 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.499875\n",
      "Step 59: Loss 116.82759094238281 Avg Reward: 497.6875 Avg Episode Len: 497.6875\n",
      "Batch size: 15926 Avg action: 0.5005625\n",
      "Step 60: Loss 118.58708953857422 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.5005\n",
      "Step 61: Loss 116.92815399169922 Avg Reward: 487.84375 Avg Episode Len: 487.84375\n",
      "Batch size: 15611 Avg action: 0.4998125\n",
      "Step 62: Loss 117.8510513305664 Avg Reward: 497.875 Avg Episode Len: 497.875\n",
      "Batch size: 15932 Avg action: 0.4998125\n",
      "Step 63: Loss 117.17239379882812 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4995625\n",
      "Step 64: Loss 115.49238586425781 Avg Reward: 475.90625 Avg Episode Len: 475.90625\n",
      "Batch size: 15229 Avg action: 0.4994375\n",
      "Step 65: Loss 116.57035827636719 Avg Reward: 491.25 Avg Episode Len: 491.25\n",
      "Batch size: 15720 Avg action: 0.499875\n",
      "Step 66: Loss 116.33820343017578 Avg Reward: 482.75 Avg Episode Len: 482.75\n",
      "Batch size: 15448 Avg action: 0.5005625\n",
      "Step 67: Loss 116.34144592285156 Avg Reward: 483.25 Avg Episode Len: 483.25\n",
      "Batch size: 15464 Avg action: 0.5001875\n",
      "Step 68: Loss 117.11038208007812 Avg Reward: 490.40625 Avg Episode Len: 490.40625\n",
      "Batch size: 15693 Avg action: 0.5000625\n",
      "Step 69: Loss 116.4996109008789 Avg Reward: 484.5625 Avg Episode Len: 484.5625\n",
      "Batch size: 15506 Avg action: 0.50025\n",
      "Step 70: Loss 120.24740600585938 Avg Reward: 490.3125 Avg Episode Len: 490.3125\n",
      "Batch size: 15690 Avg action: 0.5003125\n",
      "Step 71: Loss 119.49584197998047 Avg Reward: 479.46875 Avg Episode Len: 479.46875\n",
      "Batch size: 15343 Avg action: 0.5005\n",
      "Step 72: Loss 121.98014068603516 Avg Reward: 490.1875 Avg Episode Len: 490.1875\n",
      "Batch size: 15686 Avg action: 0.500875\n",
      "Step 73: Loss 123.98423767089844 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.5004375\n",
      "Step 74: Loss 124.03962707519531 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.49975\n",
      "Step 75: Loss 124.2208480834961 Avg Reward: 487.90625 Avg Episode Len: 487.90625\n",
      "Batch size: 15613 Avg action: 0.49975\n",
      "Step 76: Loss 125.1421127319336 Avg Reward: 485.96875 Avg Episode Len: 485.96875\n",
      "Batch size: 15551 Avg action: 0.500125\n",
      "Step 77: Loss 128.98748779296875 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4995\n",
      "Step 78: Loss 128.55398559570312 Avg Reward: 492.90625 Avg Episode Len: 492.90625\n",
      "Batch size: 15773 Avg action: 0.500625\n",
      "Step 79: Loss 129.8015594482422 Avg Reward: 492.53125 Avg Episode Len: 492.53125\n",
      "Batch size: 15761 Avg action: 0.5\n",
      "Step 80: Loss 96.6069107055664 Avg Reward: 346.59375 Avg Episode Len: 346.59375\n",
      "Batch size: 11091 Avg action: 0.50475\n",
      "Step 81: Loss 74.00525665283203 Avg Reward: 267.53125 Avg Episode Len: 267.53125\n",
      "Batch size: 8561 Avg action: 0.508572319201995\n",
      "Step 82: Loss 62.82429122924805 Avg Reward: 238.5 Avg Episode Len: 238.5\n",
      "Batch size: 7632 Avg action: 0.510625\n",
      "Step 83: Loss 60.54840850830078 Avg Reward: 225.78125 Avg Episode Len: 225.78125\n",
      "Batch size: 7225 Avg action: 0.5125853242320819\n",
      "Step 84: Loss 59.638057708740234 Avg Reward: 221.1875 Avg Episode Len: 221.1875\n",
      "Batch size: 7078 Avg action: 0.5142348754448398\n",
      "Step 85: Loss 61.61399841308594 Avg Reward: 223.75 Avg Episode Len: 223.75\n",
      "Batch size: 7160 Avg action: 0.511326860841424\n",
      "Step 86: Loss 67.74284362792969 Avg Reward: 244.71875 Avg Episode Len: 244.71875\n",
      "Batch size: 7831 Avg action: 0.5104472140762464\n",
      "Step 87: Loss 78.68758392333984 Avg Reward: 290.21875 Avg Episode Len: 290.21875\n",
      "Batch size: 9287 Avg action: 0.5081730769230769\n",
      "Step 88: Loss 113.97805786132812 Avg Reward: 414.0625 Avg Episode Len: 414.0625\n",
      "Batch size: 13250 Avg action: 0.5039375\n",
      "Step 89: Loss 129.09083557128906 Avg Reward: 479.5625 Avg Episode Len: 479.5625\n",
      "Batch size: 15346 Avg action: 0.4998125\n",
      "Step 90: Loss 132.19869995117188 Avg Reward: 485.4375 Avg Episode Len: 485.4375\n",
      "Batch size: 15534 Avg action: 0.499625\n",
      "Step 91: Loss 129.39117431640625 Avg Reward: 476.21875 Avg Episode Len: 476.21875\n",
      "Batch size: 15239 Avg action: 0.5001875\n",
      "Step 92: Loss 128.7369842529297 Avg Reward: 473.09375 Avg Episode Len: 473.09375\n",
      "Batch size: 15139 Avg action: 0.50025\n",
      "Step 93: Loss 126.52012634277344 Avg Reward: 479.5625 Avg Episode Len: 479.5625\n",
      "Batch size: 15346 Avg action: 0.50075\n",
      "Step 94: Loss 127.86967468261719 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4994375\n",
      "Step 95: Loss 126.6088638305664 Avg Reward: 496.125 Avg Episode Len: 496.125\n",
      "Batch size: 15876 Avg action: 0.5001875\n",
      "Step 96: Loss 126.51166534423828 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4996875\n",
      "Step 97: Loss 123.76055908203125 Avg Reward: 492.6875 Avg Episode Len: 492.6875\n",
      "Batch size: 15766 Avg action: 0.4994375\n",
      "Step 98: Loss 125.53717041015625 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.499875\n",
      "Step 99: Loss 124.74673461914062 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4996875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(MLP(\n",
       "   layers=[\n",
       "     Linear(\n",
       "       weight=f32[32,4],\n",
       "       bias=f32[32],\n",
       "       in_features=4,\n",
       "       out_features=32,\n",
       "       use_bias=True\n",
       "     ),\n",
       "     Linear(\n",
       "       weight=f32[2,32],\n",
       "       bias=f32[2],\n",
       "       in_features=32,\n",
       "       out_features=2,\n",
       "       use_bias=True\n",
       "     )\n",
       "   ],\n",
       "   output_proj=Linear(\n",
       "     weight=f32[2,2],\n",
       "     bias=f32[2],\n",
       "     in_features=2,\n",
       "     out_features=2,\n",
       "     use_bias=True\n",
       "   )\n",
       " ),\n",
       " (ScaleByAdamState(count=Array(100, dtype=int32), mu=MLP(\n",
       "    layers=[\n",
       "      Linear(\n",
       "        weight=f32[32,4],\n",
       "        bias=f32[32],\n",
       "        in_features=4,\n",
       "        out_features=32,\n",
       "        use_bias=True\n",
       "      ),\n",
       "      Linear(\n",
       "        weight=f32[2,32],\n",
       "        bias=f32[2],\n",
       "        in_features=32,\n",
       "        out_features=2,\n",
       "        use_bias=True\n",
       "      )\n",
       "    ],\n",
       "    output_proj=Linear(\n",
       "      weight=f32[2,2],\n",
       "      bias=f32[2],\n",
       "      in_features=2,\n",
       "      out_features=2,\n",
       "      use_bias=True\n",
       "    )\n",
       "  ), nu=MLP(\n",
       "    layers=[\n",
       "      Linear(\n",
       "        weight=f32[32,4],\n",
       "        bias=f32[32],\n",
       "        in_features=4,\n",
       "        out_features=32,\n",
       "        use_bias=True\n",
       "      ),\n",
       "      Linear(\n",
       "        weight=f32[2,32],\n",
       "        bias=f32[2],\n",
       "        in_features=32,\n",
       "        out_features=2,\n",
       "        use_bias=True\n",
       "      )\n",
       "    ],\n",
       "    output_proj=Linear(\n",
       "      weight=f32[2,2],\n",
       "      bias=f32[2],\n",
       "      in_features=2,\n",
       "      out_features=2,\n",
       "      use_bias=True\n",
       "    )\n",
       "  )),\n",
       "  EmptyState()))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(envs, model, optim, params, jax.random.PRNGKey(812342))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db2044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6748e5e-6f3e-41fb-b66c-9eaf29adb522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rl-video-episode-0.mp4',\n",
       " 'rl-video-episode-1.mp4',\n",
       " 'rl-video-episode-10.mp4',\n",
       " 'rl-video-episode-11.mp4',\n",
       " 'rl-video-episode-12.mp4',\n",
       " 'rl-video-episode-13.mp4',\n",
       " 'rl-video-episode-14.mp4',\n",
       " 'rl-video-episode-2.mp4',\n",
       " 'rl-video-episode-3.mp4',\n",
       " 'rl-video-episode-4.mp4',\n",
       " 'rl-video-episode-5.mp4',\n",
       " 'rl-video-episode-6.mp4',\n",
       " 'rl-video-episode-7.mp4',\n",
       " 'rl-video-episode-8.mp4',\n",
       " 'rl-video-episode-9.mp4']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(os.listdir(\"./save_videos2\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
