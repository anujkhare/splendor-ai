{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "736a259e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLearning Notes\\n\\nI messed up a whole bunch of things in this code vs https://github.com/openai/spinningup/blob/master/spinup/examples/pytorch/pg_math/1_simple_pg.py:\\n1. Reward normalization\\n2. Didn\\'t use \"Categorical\" distribution for the policy\\n3. Loss is something negative and huge...\\n4. Sampled from softmax using argmax - should have used torch.distributions.Categorical or jax.random.categorical -- which uses gumbel softmax trick to make it differentiable\\n5. Model was huge - i used 512x512x2 vs 32x2\\n6. LR was too small - 1e-4 vs 1e-2\\n7. Batch size was too small - 32 vs 5000 -- also samples are each time step, not each episode\\n8. INCORRECT OBSERVATIONS USED IN THE EXPERIENCE BUFFER - I used the newly sampled ones after running env.step\\n   but this was incorrect, since I needed use the previous ones...\\n \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Learning Notes\n",
    "\n",
    "I messed up a whole bunch of things in this code vs https://github.com/openai/spinningup/blob/master/spinup/examples/pytorch/pg_math/1_simple_pg.py:\n",
    "1. Reward normalization\n",
    "2. Didn't use \"Categorical\" distribution for the policy\n",
    "3. Loss is something negative and huge...\n",
    "4. Sampled from softmax using argmax - should have used torch.distributions.Categorical or jax.random.categorical -- which uses gumbel softmax trick to make it differentiable\n",
    "5. Model was huge - i used 512x512x2 vs 32x2\n",
    "6. LR was too small - 1e-4 vs 1e-2\n",
    "7. Batch size was too small - 32 vs 5000 -- also samples are each time step, not each episode\n",
    "8. INCORRECT OBSERVATIONS USED IN THE EXPERIENCE BUFFER - I used the newly sampled ones after running env.step\n",
    "   but this was incorrect, since I needed use the previous ones...\n",
    " \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0be9fb82-9b9c-4eb8-bd12-44aee0215ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import optax\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from gymnasium import wrappers\n",
    "from tensorflow_probability.substrates import jax as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ea56f6a-916d-420b-9456-8d1544e8b91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For inference:\n",
    "# # The environment is described in the following link:\n",
    "# # https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
    "# env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "\n",
    "# # Record videos\n",
    "# trigger = lambda _: True\n",
    "# env = wrappers.RecordVideo(env, video_folder=\"./save_videos2\", episode_trigger=trigger, disable_logger=True, video_length=1000)\n",
    "# env = wrappers.RecordEpisodeStatistics(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f0b4f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import enum\n",
    "\n",
    "\n",
    "class PolicyGradientWeight(enum.Enum):\n",
    "    NONE = 0\n",
    "    TRAJECTORY_REWARDS = 1\n",
    "    REWARDS_TO_GO = 2\n",
    "\n",
    "\n",
    "class PolicyGradientWeightBaseline(enum.Enum):\n",
    "    NONE = 0\n",
    "    AVERAGE_TRAJECTORY_REWARDS = 1\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Params:\n",
    "    learning_rate: float = 1e-2\n",
    "    batch_size: int = 4096  # Use a large batch size for policy iteration!\n",
    "    max_steps: int = 100\n",
    "    obs_dim: int = 4\n",
    "\n",
    "    # model related\n",
    "    hidden_dims: list[int] = dataclasses.field(default_factory=lambda: [32])\n",
    "\n",
    "    # Algorithm hypers\n",
    "    policy_gradient_weight: PolicyGradientWeight = PolicyGradientWeight.TRAJECTORY_REWARDS\n",
    "    policy_gradient_weight_baseline: PolicyGradientWeightBaseline = PolicyGradientWeightBaseline.NONE\n",
    "\n",
    "    # initial exploration rate\n",
    "    epsilon: int = 0.15\n",
    "    # epsilon is decayed over time with a cosine schedule, starting at `epsilon`\n",
    "    # and ending at `epsilon_min` over `max_steps` steps.\n",
    "    epsilon_min: float = 0.1  # always explore at least 10% of the time\n",
    "\n",
    "    def get_epsilon(self, step: int) -> float:\n",
    "        if step > self.max_steps:\n",
    "            raise ValueError(\"step must be less than max_steps\")\n",
    "        return self.epsilon_min + (self.epsilon - self.epsilon_min) * (1 + np.cos(np.pi * step / self.max_steps) ) / 2 \n",
    "\n",
    "\n",
    "params = Params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9c21827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define the policy\n",
    "import equinox as eqx\n",
    "import jax\n",
    "\n",
    "class MLP(eqx.Module):\n",
    "    layers: list\n",
    "    output_proj: eqx.nn.Linear\n",
    "\n",
    "    def __init__(\n",
    "        self, in_size: int, hidden_dims: list[int], out_size: int, key: jax.random.PRNGKey):\n",
    "        self.layers = []\n",
    "        prev_dim = in_size\n",
    "        for dim in hidden_dims:\n",
    "            key, layer_key = jax.random.split(key)\n",
    "            self.layers.append(\n",
    "                eqx.nn.Linear(\n",
    "                    in_features=prev_dim,\n",
    "                    out_features=dim,\n",
    "                    use_bias=True,\n",
    "                    key=layer_key\n",
    "                )\n",
    "            )\n",
    "            prev_dim = dim\n",
    "        self.output_proj = eqx.nn.Linear(\n",
    "            in_features=prev_dim,\n",
    "            out_features=out_size,\n",
    "            use_bias=True,\n",
    "            key=key\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            x = jax.nn.relu(x)\n",
    "        # log probs (log(softmax(x))\n",
    "        x = self.output_proj(x)\n",
    "        return x\n",
    "        # return x - jax.scipy.special.logsumexp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69566410",
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def policy(model: MLP, obs: np.ndarray, epsilon: float, key: jax.random.PRNGKey):\n",
    "    del epsilon  # unused\n",
    "    # Find best action based on policy model\n",
    "    logits = jax.vmap(model)(obs)\n",
    "    # unnormalized log-probabilities are represented by the model output logits -- treat as categorical distribution\n",
    "    model_actions = tfp.distributions.Categorical(logits=logits).sample(seed=key)\n",
    "\n",
    "    # # maybe explore based on epsilon-greedy strategy\n",
    "    # p_explore = jax.random.uniform(key1, (len(obs),))\n",
    "    # random_actions = jax.random.randint(key2, (len(obs),), minval=0, maxval=2)\n",
    "\n",
    "    # final_actions = jax.numpy.where(p_explore < epsilon, random_actions, model_actions)\n",
    "    final_actions = model_actions\n",
    "    return final_actions, model_actions\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class SampleOutput:\n",
    "    observations: np.ndarray = dataclasses.field(default_factory=lambda: np.array([[]]))\n",
    "    actions: np.ndarray = dataclasses.field(default_factory=lambda: np.array([]))\n",
    "    rewards: np.ndarray = dataclasses.field(default_factory=lambda: np.array([]))\n",
    "    rewards_to_go: np.ndarray = dataclasses.field(default_factory=lambda: np.array([]))\n",
    "    trajectory_rewards: np.ndarray = dataclasses.field(default_factory=lambda: np.array([]))\n",
    "    valid: np.ndarray = dataclasses.field(default_factory=lambda: np.array([]))\n",
    "    avg_expected_reward: float = 0.0\n",
    "    avg_action: float = 0.0\n",
    "    avg_episode_length: float = 0.0\n",
    "    num_episodes: int = 0\n",
    "\n",
    "    def num_samples(self):\n",
    "        return len(self.observations)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        def _weighted_sum(attr):\n",
    "            return (getattr(self, attr) * self.num_samples() + getattr(other, attr) * other.num_samples()) / (self.num_samples() + other.num_samples())\n",
    "        return SampleOutput(\n",
    "            observations=np.concatenate([self.observations, other.observations]),\n",
    "            actions=np.concatenate([self.actions, other.actions]),\n",
    "            rewards=np.concatenate([self.rewards, other.rewards]),\n",
    "            rewards_to_go=np.concatenate([self.rewards_to_go, other.rewards_to_go]),\n",
    "            trajectory_rewards=np.concatenate([self.trajectory_rewards, other.trajectory_rewards]),\n",
    "            valid=np.concatenate([self.valid, other.valid]),\n",
    "            num_episodes=self.num_episodes + other.num_episodes,\n",
    "            avg_expected_reward=_weighted_sum('avg_expected_reward'),\n",
    "            avg_action=_weighted_sum('avg_action'),\n",
    "            avg_episode_length=_weighted_sum('avg_episode_length')\n",
    "        )\n",
    "\n",
    "def sample_from_policy(envs: gym.vector.VectorEnv, model: MLP, epsilon: float, key: jax.random.PRNGKey):\n",
    "    \"\"\"Run all envs in parallel, return the trajectory of each env.\"\"\"\n",
    "    obs, _ = envs.reset()\n",
    "    episode_over = np.array([False] * envs.num_envs)\n",
    "\n",
    "    # Track the actions, rewards, and gradients for policy optimization\n",
    "    trajectory_observations = []\n",
    "    trajectory_rewards = []\n",
    "    trajectory_actions = []\n",
    "    trajectory_valid = []\n",
    "\n",
    "    # iterate through the vectorized environments\n",
    "    while np.any(~episode_over):\n",
    "        trajectory_observations.append(obs)\n",
    "\n",
    "        key_used, key = jax.random.split(key)\n",
    "        actions, _ = policy(model, obs, epsilon, key_used)\n",
    "        obs, rewards, terminated, truncated, _ = envs.step(np.array(actions))\n",
    "        episode_over = episode_over | terminated | truncated\n",
    "\n",
    "        # Mask out everything for episodes that are over\n",
    "        mask = ~episode_over\n",
    "        trajectory_rewards.append(rewards)\n",
    "        trajectory_actions.append(actions)\n",
    "        trajectory_valid.append(mask)\n",
    "\n",
    "    # [B, T, D]\n",
    "    trajectory_observations = np.einsum('Tbd->bTd', np.stack(trajectory_observations))\n",
    "    # [B, T]\n",
    "    trajectory_actions = np.stack(trajectory_actions).T\n",
    "    trajectory_rewards = np.stack(trajectory_rewards).T\n",
    "    trajectory_valid = np.stack(trajectory_valid).T\n",
    "\n",
    "    # compute the rewards to go [B, T]\n",
    "    rewards_to_go = np.where(trajectory_valid, trajectory_rewards, 0)[:, ::-1].cumsum(axis=-1)[:, ::-1]\n",
    "    # Trajectory rewards [B, T]\n",
    "    rewards_per_trajectory = rewards_to_go[:, :1].repeat(rewards_to_go.shape[-1], axis=-1)\n",
    "\n",
    "    avg_expected_reward = np.mean(rewards_to_go[:, 0])  # average expected rewards for trajectories\n",
    "    avg_action = np.mean(trajectory_actions)\n",
    "    avg_episode_length = np.sum(trajectory_valid, axis=-1).mean()\n",
    "    num_episodes = len(trajectory_observations)\n",
    "\n",
    "    # Flatten all the (state, action, reward) tuples across episodes \n",
    "    # it doesn't matter which episode they came from (as long as we keep a track of the rewards!)\n",
    "    trajectory_valid = trajectory_valid.ravel()  # (T*B, )\n",
    "\n",
    "    return SampleOutput(\n",
    "        # [B * T, D]\n",
    "        observations=trajectory_observations.reshape(-1, trajectory_observations.shape[-1])[trajectory_valid],\n",
    "        # [B * T,]\n",
    "        actions=trajectory_actions.ravel()[trajectory_valid],\n",
    "        rewards=trajectory_rewards.ravel()[trajectory_valid],\n",
    "        rewards_to_go=rewards_to_go.ravel()[trajectory_valid],\n",
    "        trajectory_rewards=rewards_per_trajectory.ravel()[trajectory_valid],\n",
    "        valid=trajectory_valid,\n",
    "        # []\n",
    "        avg_expected_reward=avg_expected_reward,\n",
    "        avg_action=avg_action,\n",
    "        avg_episode_length=avg_episode_length,\n",
    "        num_episodes=num_episodes\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "034071f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, obs, actions, weights):\n",
    "    logits = jax.vmap(model)(obs)\n",
    "    log_probs = tfp.distributions.Categorical(logits=logits).log_prob(actions)\n",
    "\n",
    "    # the loss is technically the log likelihood of the actions weighted by the rewards-to-go\n",
    "    return -jax.numpy.sum(log_probs * weights) / len(obs)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def update_model(model, obs, actions, weights, optimizer, opt_state):\n",
    "    loss, grad = eqx.filter_value_and_grad(loss_fn)(model, obs, actions, weights)\n",
    "    updates, opt_state = optimizer.update(grad, opt_state, eqx.filter(model, eqx.is_array))\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "152a5081",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class SampleAndUpdatePolicyOutputs:\n",
    "    model: MLP\n",
    "    opt_state: optax.OptState\n",
    "    loss: float\n",
    "    avg_expected_reward: float\n",
    "    avg_action: float\n",
    "    avg_episode_length: float\n",
    "    num_samples: int\n",
    "\n",
    "\n",
    "def sample_batch_and_update_policy(\n",
    "    envs: gym.vector.VectorEnv,\n",
    "    model: MLP, optimizer: optax.GradientTransformation,\n",
    "    opt_state: optax.OptState,\n",
    "    params: Params,\n",
    "    step: int,\n",
    "    key: jax.random.PRNGKey):\n",
    "    # Sample at least \"batch_size\" time steps (state,action,reward)s from the policy\n",
    "    samples = None\n",
    "    while True:\n",
    "        key_used, key = jax.random.split(key)\n",
    "        new_samples = sample_from_policy(envs, model, params.get_epsilon(step), key_used)\n",
    "        if samples is None:\n",
    "            samples = new_samples\n",
    "        else:\n",
    "            samples = samples + new_samples\n",
    "        if samples.num_samples() >= params.batch_size:\n",
    "            break\n",
    "    # print(f\"Sampled {samples.num_samples()} samples from {samples.num_episodes} episodes\")\n",
    "\n",
    "    # Compute the weights for the policy gradient\n",
    "    if params.policy_gradient_weight == PolicyGradientWeight.REWARDS_TO_GO:\n",
    "        weights = samples.rewards_to_go\n",
    "    elif params.policy_gradient_weight == PolicyGradientWeight.TRAJECTORY_REWARDS:\n",
    "        weights = samples.trajectory_rewards\n",
    "    else:\n",
    "        raise ValueError\n",
    "    \n",
    "    if params.policy_gradient_weight_baseline == PolicyGradientWeightBaseline.AVERAGE_TRAJECTORY_REWARDS:\n",
    "        weights -= samples.avg_expected_reward\n",
    "\n",
    "    # Compute the loss and the gradient\n",
    "    model, opt_state, loss = update_model(\n",
    "        model=model,\n",
    "        obs=samples.observations,\n",
    "        actions=samples.actions,\n",
    "        weights=weights,\n",
    "        optimizer=optimizer,\n",
    "        opt_state=opt_state)\n",
    "    return SampleAndUpdatePolicyOutputs(\n",
    "        model=model, opt_state=opt_state,\n",
    "        loss=loss,\n",
    "        avg_expected_reward=samples.avg_expected_reward,\n",
    "        avg_action=samples.avg_action,\n",
    "        avg_episode_length=samples.avg_episode_length,\n",
    "        num_samples=samples.num_samples(),\n",
    "    )\n",
    "\n",
    "\n",
    "def train(envs, model, optimizer, params, key: jax.random.PRNGKey):\n",
    "    opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n",
    "\n",
    "    for step in range(params.max_steps):\n",
    "        key_used, key = jax.random.split(key)\n",
    "        outputs = sample_batch_and_update_policy(\n",
    "            envs, model, optimizer, opt_state, params, step, key_used)\n",
    "        model, opt_state = outputs.model, outputs.opt_state\n",
    "        if step % 1 == 0:\n",
    "            print(f\"Step {step}: Loss {outputs.loss} Avg Reward: {outputs.avg_expected_reward} Avg Episode Len: {outputs.avg_episode_length}\")\n",
    "            print(f\"Batch size: {outputs.num_samples} Avg action: {outputs.avg_action}\")\n",
    "            # print(f\"New epsilon: {params.get_epsilon(step)}\")\n",
    "    return model, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4c5dcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss 16.650728225708008 Avg Reward: 19.61086665527455 Avg Episode Len: 19.61086665527455\n",
      "Batch size: 4389 Avg action: 0.5666989693481507\n",
      "Step 1: Loss 19.992151260375977 Avg Reward: 22.439758329422805 Avg Episode Len: 22.439758329422805\n",
      "Batch size: 4262 Avg action: 0.5551792231824751\n",
      "Step 2: Loss 22.345544815063477 Avg Reward: 21.881024368231046 Avg Episode Len: 21.881024368231046\n",
      "Batch size: 4155 Avg action: 0.5419483606281343\n",
      "Step 3: Loss 20.842893600463867 Avg Reward: 23.478734294368408 Avg Episode Len: 23.478734294368408\n",
      "Batch size: 4457 Avg action: 0.5350342698876588\n",
      "Step 4: Loss 23.622314453125 Avg Reward: 25.822790189412338 Avg Episode Len: 25.822790189412338\n",
      "Batch size: 4118 Avg action: 0.5314920562482143\n",
      "Step 5: Loss 26.265615463256836 Avg Reward: 27.89850113122172 Avg Episode Len: 27.89850113122172\n",
      "Batch size: 4420 Avg action: 0.5288746647456954\n",
      "Step 6: Loss 25.197097778320312 Avg Reward: 28.822456538377434 Avg Episode Len: 28.822456538377434\n",
      "Batch size: 4573 Avg action: 0.5188186506152299\n",
      "Step 7: Loss 25.387235641479492 Avg Reward: 29.63429017952554 Avg Episode Len: 29.63429017952554\n",
      "Batch size: 4679 Avg action: 0.5188910555420002\n",
      "Step 8: Loss 23.812280654907227 Avg Reward: 28.43800200803213 Avg Episode Len: 28.43800200803213\n",
      "Batch size: 4482 Avg action: 0.5204452671671451\n",
      "Step 9: Loss 31.32577896118164 Avg Reward: 35.16910203397407 Avg Episode Len: 35.16910203397407\n",
      "Batch size: 4474 Avg action: 0.5140806723750133\n",
      "Step 10: Loss 29.20964813232422 Avg Reward: 33.952337609801205 Avg Episode Len: 33.952337609801205\n",
      "Batch size: 4326 Avg action: 0.5107889880060111\n",
      "Step 11: Loss 32.85026168823242 Avg Reward: 38.15933261405672 Avg Episode Len: 38.15933261405672\n",
      "Batch size: 4866 Avg action: 0.505157030221495\n",
      "Step 12: Loss 35.10686111450195 Avg Reward: 40.78774195093906 Avg Episode Len: 40.78774195093906\n",
      "Batch size: 5218 Avg action: 0.5028457215705512\n",
      "Step 13: Loss 31.017234802246094 Avg Reward: 38.96320787988714 Avg Episode Len: 38.96320787988714\n",
      "Batch size: 4962 Avg action: 0.5082858055552341\n",
      "Step 14: Loss 40.5281982421875 Avg Reward: 45.19783911121452 Avg Episode Len: 45.19783911121452\n",
      "Batch size: 4298 Avg action: 0.507217430887035\n",
      "Step 15: Loss 35.813499450683594 Avg Reward: 45.950048207803995 Avg Episode Len: 45.950048207803995\n",
      "Batch size: 4408 Avg action: 0.5037706014799843\n",
      "Step 16: Loss 39.36763000488281 Avg Reward: 50.78687435500516 Avg Episode Len: 50.78687435500516\n",
      "Batch size: 4845 Avg action: 0.5030268741663017\n",
      "Step 17: Loss 48.0860595703125 Avg Reward: 60.207019030239834 Avg Episode Len: 60.207019030239834\n",
      "Batch size: 5754 Avg action: 0.501630697015775\n",
      "Step 18: Loss 43.86500930786133 Avg Reward: 55.244827178030306 Avg Episode Len: 55.244827178030306\n",
      "Batch size: 5280 Avg action: 0.5015040604429748\n",
      "Step 19: Loss 43.97807693481445 Avg Reward: 54.363735604606525 Avg Episode Len: 54.363735604606525\n",
      "Batch size: 5210 Avg action: 0.5009450865044965\n",
      "Step 20: Loss 41.454002380371094 Avg Reward: 55.61187734962406 Avg Episode Len: 55.61187734962406\n",
      "Batch size: 5320 Avg action: 0.4986689198804104\n",
      "Step 21: Loss 40.963768005371094 Avg Reward: 56.67545771075447 Avg Episode Len: 56.67545771075447\n",
      "Batch size: 5421 Avg action: 0.4981986238166689\n",
      "Step 22: Loss 49.86886978149414 Avg Reward: 66.06839651389566 Avg Episode Len: 66.06839651389566\n",
      "Batch size: 4102 Avg action: 0.4994633287423907\n",
      "Step 23: Loss 48.880348205566406 Avg Reward: 66.64871776084408 Avg Episode Len: 66.64871776084408\n",
      "Batch size: 4265 Avg action: 0.493146611702941\n",
      "Step 24: Loss 52.26862716674805 Avg Reward: 70.93866360089186 Avg Episode Len: 70.93866360089186\n",
      "Batch size: 4485 Avg action: 0.49446267479813205\n",
      "Step 25: Loss 47.824066162109375 Avg Reward: 71.36675974594831 Avg Episode Len: 71.36675974594831\n",
      "Batch size: 4566 Avg action: 0.49416345053132954\n",
      "Step 26: Loss 50.201168060302734 Avg Reward: 76.61363345941525 Avg Episode Len: 76.61363345941525\n",
      "Batch size: 4891 Avg action: 0.4983282580802966\n",
      "Step 27: Loss 60.53083419799805 Avg Reward: 85.31074283351708 Avg Episode Len: 85.31074283351708\n",
      "Batch size: 5442 Avg action: 0.49899593050564894\n",
      "Step 28: Loss 57.03491973876953 Avg Reward: 87.79629361758946 Avg Episode Len: 87.79629361758946\n",
      "Batch size: 5617 Avg action: 0.49112228187996404\n",
      "Step 29: Loss 63.148193359375 Avg Reward: 96.48090965346535 Avg Episode Len: 96.48090965346535\n",
      "Batch size: 6060 Avg action: 0.49528054398103905\n",
      "Step 30: Loss 73.45555877685547 Avg Reward: 103.12257424426554 Avg Episode Len: 103.12257424426554\n",
      "Batch size: 6583 Avg action: 0.49874284324335855\n",
      "Step 31: Loss 65.09123229980469 Avg Reward: 101.49957403965304 Avg Episode Len: 101.49957403965304\n",
      "Batch size: 6456 Avg action: 0.5041477593962572\n",
      "Step 32: Loss 67.2674789428711 Avg Reward: 103.1636169201521 Avg Episode Len: 103.1636169201521\n",
      "Batch size: 6575 Avg action: 0.5096015942899073\n",
      "Step 33: Loss 79.36604309082031 Avg Reward: 117.41182602143336 Avg Episode Len: 117.41182602143336\n",
      "Batch size: 7465 Avg action: 0.5087699052471957\n",
      "Step 34: Loss 75.48361206054688 Avg Reward: 118.04899129484973 Avg Episode Len: 118.04899129484973\n",
      "Batch size: 7553 Avg action: 0.5086389181628397\n",
      "Step 35: Loss 96.52519989013672 Avg Reward: 136.9375 Avg Episode Len: 136.9375\n",
      "Batch size: 4382 Avg action: 0.5115899725274725\n",
      "Step 36: Loss 91.05004119873047 Avg Reward: 150.84375 Avg Episode Len: 150.84375\n",
      "Batch size: 4827 Avg action: 0.5070180084745762\n",
      "Step 37: Loss 95.82147216796875 Avg Reward: 152.40625 Avg Episode Len: 152.40625\n",
      "Batch size: 4877 Avg action: 0.5084459459459459\n",
      "Step 38: Loss 113.6757583618164 Avg Reward: 171.0 Avg Episode Len: 171.0\n",
      "Batch size: 5472 Avg action: 0.51375\n",
      "Step 39: Loss 115.54352569580078 Avg Reward: 173.71875 Avg Episode Len: 173.71875\n",
      "Batch size: 5559 Avg action: 0.5040760869565217\n",
      "Step 40: Loss 141.26263427734375 Avg Reward: 204.90625 Avg Episode Len: 204.90625\n",
      "Batch size: 6557 Avg action: 0.49025\n",
      "Step 41: Loss 114.6560287475586 Avg Reward: 189.0625 Avg Episode Len: 189.0625\n",
      "Batch size: 6050 Avg action: 0.4854872881355932\n",
      "Step 42: Loss 105.73432922363281 Avg Reward: 170.1875 Avg Episode Len: 170.1875\n",
      "Batch size: 5446 Avg action: 0.4767530487804878\n",
      "Step 43: Loss 99.3938980102539 Avg Reward: 162.78125 Avg Episode Len: 162.78125\n",
      "Batch size: 5209 Avg action: 0.47221057046979864\n",
      "Step 44: Loss 91.38076782226562 Avg Reward: 152.125 Avg Episode Len: 152.125\n",
      "Batch size: 4868 Avg action: 0.4699909747292419\n",
      "Step 45: Loss 106.16899108886719 Avg Reward: 171.15625 Avg Episode Len: 171.15625\n",
      "Batch size: 5477 Avg action: 0.4726216814159292\n",
      "Step 46: Loss 124.4246826171875 Avg Reward: 212.75 Avg Episode Len: 212.75\n",
      "Batch size: 6808 Avg action: 0.47844551282051284\n",
      "Step 47: Loss 196.59275817871094 Avg Reward: 318.375 Avg Episode Len: 318.375\n",
      "Batch size: 10188 Avg action: 0.4888125\n",
      "Step 48: Loss 230.58363342285156 Avg Reward: 385.59375 Avg Episode Len: 385.59375\n",
      "Batch size: 12339 Avg action: 0.4966875\n",
      "Step 49: Loss 216.52658081054688 Avg Reward: 353.21875 Avg Episode Len: 353.21875\n",
      "Batch size: 11303 Avg action: 0.5041875\n",
      "Step 50: Loss 219.47463989257812 Avg Reward: 362.34375 Avg Episode Len: 362.34375\n",
      "Batch size: 11595 Avg action: 0.5030625\n",
      "Step 51: Loss 223.5909881591797 Avg Reward: 386.375 Avg Episode Len: 386.375\n",
      "Batch size: 12364 Avg action: 0.5036875\n",
      "Step 52: Loss 206.9698028564453 Avg Reward: 337.46875 Avg Episode Len: 337.46875\n",
      "Batch size: 10799 Avg action: 0.4994375\n",
      "Step 53: Loss 228.14208984375 Avg Reward: 392.21875 Avg Episode Len: 392.21875\n",
      "Batch size: 12551 Avg action: 0.5004375\n",
      "Step 54: Loss 218.2354278564453 Avg Reward: 362.28125 Avg Episode Len: 362.28125\n",
      "Batch size: 11593 Avg action: 0.498875\n",
      "Step 55: Loss 213.1039581298828 Avg Reward: 356.9375 Avg Episode Len: 356.9375\n",
      "Batch size: 11422 Avg action: 0.4991875\n",
      "Step 56: Loss 224.11085510253906 Avg Reward: 374.5625 Avg Episode Len: 374.5625\n",
      "Batch size: 11986 Avg action: 0.499875\n",
      "Step 57: Loss 217.90625 Avg Reward: 352.59375 Avg Episode Len: 352.59375\n",
      "Batch size: 11283 Avg action: 0.496375\n",
      "Step 58: Loss 153.3314208984375 Avg Reward: 249.875 Avg Episode Len: 249.875\n",
      "Batch size: 7996 Avg action: 0.4932255244755245\n",
      "Step 59: Loss 140.8050994873047 Avg Reward: 236.21875 Avg Episode Len: 236.21875\n",
      "Batch size: 7559 Avg action: 0.49203725961538464\n",
      "Step 60: Loss 136.42787170410156 Avg Reward: 225.84375 Avg Episode Len: 225.84375\n",
      "Batch size: 7227 Avg action: 0.4915674603174603\n",
      "Step 61: Loss 146.05862426757812 Avg Reward: 235.75 Avg Episode Len: 235.75\n",
      "Batch size: 7544 Avg action: 0.4890934405940594\n",
      "Step 62: Loss 140.6422119140625 Avg Reward: 212.03125 Avg Episode Len: 212.03125\n",
      "Batch size: 6785 Avg action: 0.4938279625779626\n",
      "Step 63: Loss 147.57740783691406 Avg Reward: 225.78125 Avg Episode Len: 225.78125\n",
      "Batch size: 7225 Avg action: 0.49217002237136465\n",
      "Step 64: Loss 168.40602111816406 Avg Reward: 254.6875 Avg Episode Len: 254.6875\n",
      "Batch size: 8150 Avg action: 0.4916281755196305\n",
      "Step 65: Loss 156.55213928222656 Avg Reward: 235.5625 Avg Episode Len: 235.5625\n",
      "Batch size: 7538 Avg action: 0.4925\n",
      "Step 66: Loss 174.7646942138672 Avg Reward: 243.28125 Avg Episode Len: 243.28125\n",
      "Batch size: 7785 Avg action: 0.4943125\n",
      "Step 67: Loss 203.2625274658203 Avg Reward: 287.3125 Avg Episode Len: 287.3125\n",
      "Batch size: 9194 Avg action: 0.496875\n",
      "Step 68: Loss 215.15956115722656 Avg Reward: 338.8125 Avg Episode Len: 338.8125\n",
      "Batch size: 10842 Avg action: 0.4974375\n",
      "Step 69: Loss 237.48703002929688 Avg Reward: 369.84375 Avg Episode Len: 369.84375\n",
      "Batch size: 11835 Avg action: 0.4991875\n",
      "Step 70: Loss 242.4466552734375 Avg Reward: 403.34375 Avg Episode Len: 403.34375\n",
      "Batch size: 12907 Avg action: 0.4993125\n",
      "Step 71: Loss 255.2688446044922 Avg Reward: 450.09375 Avg Episode Len: 450.09375\n",
      "Batch size: 14403 Avg action: 0.499125\n",
      "Step 72: Loss 237.30003356933594 Avg Reward: 391.625 Avg Episode Len: 391.625\n",
      "Batch size: 12532 Avg action: 0.4993125\n",
      "Step 73: Loss 251.63539123535156 Avg Reward: 451.875 Avg Episode Len: 451.875\n",
      "Batch size: 14460 Avg action: 0.499375\n",
      "Step 74: Loss 245.88551330566406 Avg Reward: 440.84375 Avg Episode Len: 440.84375\n",
      "Batch size: 14107 Avg action: 0.499625\n",
      "Step 75: Loss 249.51361083984375 Avg Reward: 471.0625 Avg Episode Len: 471.0625\n",
      "Batch size: 15074 Avg action: 0.4999375\n",
      "Step 76: Loss 249.79299926757812 Avg Reward: 470.9375 Avg Episode Len: 470.9375\n",
      "Batch size: 15070 Avg action: 0.5000625\n",
      "Step 77: Loss 252.04847717285156 Avg Reward: 484.40625 Avg Episode Len: 484.40625\n",
      "Batch size: 15501 Avg action: 0.4995625\n",
      "Step 78: Loss 256.17742919921875 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.5008125\n",
      "Step 79: Loss 254.50674438476562 Avg Reward: 495.34375 Avg Episode Len: 495.34375\n",
      "Batch size: 15851 Avg action: 0.4999375\n",
      "Step 80: Loss 250.56741333007812 Avg Reward: 488.65625 Avg Episode Len: 488.65625\n",
      "Batch size: 15637 Avg action: 0.4994375\n",
      "Step 81: Loss 253.8223114013672 Avg Reward: 494.25 Avg Episode Len: 494.25\n",
      "Batch size: 15816 Avg action: 0.49925\n",
      "Step 82: Loss 254.034912109375 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4994375\n",
      "Step 83: Loss 255.28573608398438 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4996875\n",
      "Step 84: Loss 253.63644409179688 Avg Reward: 497.28125 Avg Episode Len: 497.28125\n",
      "Batch size: 15913 Avg action: 0.4983125\n",
      "Step 85: Loss 251.30845642089844 Avg Reward: 489.5 Avg Episode Len: 489.5\n",
      "Batch size: 15664 Avg action: 0.4988125\n",
      "Step 86: Loss 247.41355895996094 Avg Reward: 487.71875 Avg Episode Len: 487.71875\n",
      "Batch size: 15607 Avg action: 0.498\n",
      "Step 87: Loss 249.97433471679688 Avg Reward: 485.96875 Avg Episode Len: 485.96875\n",
      "Batch size: 15551 Avg action: 0.498875\n",
      "Step 88: Loss 246.796875 Avg Reward: 488.75 Avg Episode Len: 488.75\n",
      "Batch size: 15640 Avg action: 0.49925\n",
      "Step 89: Loss 249.70956420898438 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.499875\n",
      "Step 90: Loss 249.91600036621094 Avg Reward: 488.5625 Avg Episode Len: 488.5625\n",
      "Batch size: 15634 Avg action: 0.4991875\n",
      "Step 91: Loss 252.8626708984375 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4996875\n",
      "Step 92: Loss 251.4920196533203 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4998125\n",
      "Step 93: Loss 252.39344787597656 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.499\n",
      "Step 94: Loss 250.1562042236328 Avg Reward: 493.34375 Avg Episode Len: 493.34375\n",
      "Batch size: 15787 Avg action: 0.498875\n",
      "Step 95: Loss 251.52334594726562 Avg Reward: 492.96875 Avg Episode Len: 492.96875\n",
      "Batch size: 15775 Avg action: 0.4985\n",
      "Step 96: Loss 239.52548217773438 Avg Reward: 461.59375 Avg Episode Len: 461.59375\n",
      "Batch size: 14771 Avg action: 0.496875\n",
      "Step 97: Loss 230.36561584472656 Avg Reward: 443.5625 Avg Episode Len: 443.5625\n",
      "Batch size: 14194 Avg action: 0.4965625\n",
      "Step 98: Loss 220.1856231689453 Avg Reward: 429.09375 Avg Episode Len: 429.09375\n",
      "Batch size: 13731 Avg action: 0.4943125\n",
      "Step 99: Loss 211.27383422851562 Avg Reward: 407.34375 Avg Episode Len: 407.34375\n",
      "Batch size: 13035 Avg action: 0.4949375\n"
     ]
    }
   ],
   "source": [
    "params = Params(\n",
    "    policy_gradient_weight=PolicyGradientWeight.TRAJECTORY_REWARDS,\n",
    "    policy_gradient_weight_baseline=PolicyGradientWeightBaseline.NONE,\n",
    ")\n",
    "# Vectorize the enviroment such that we can sample multiple trajectories concurrently\n",
    "envs = gym.make_vec(\"CartPole-v1\", num_envs=32, vectorization_mode='vector_entry_point')\n",
    "\n",
    "optim  = optax.adam(params.learning_rate)\n",
    "model = MLP(\n",
    "    in_size=4,\n",
    "    hidden_dims=params.hidden_dims + [2],\n",
    "    out_size=2,\n",
    "    key=jax.random.PRNGKey(12513)\n",
    ")\n",
    "\n",
    "model, opt_state = train(envs, model, optim, params, jax.random.PRNGKey(812342))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e7e3898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss 9.154587745666504 Avg Reward: 20.24598471422242 Avg Episode Len: 20.24598471422242\n",
      "Batch size: 4514 Avg action: 0.563946311968656\n",
      "Step 1: Loss 11.030065536499023 Avg Reward: 23.486860236220473 Avg Episode Len: 23.486860236220473\n",
      "Batch size: 4445 Avg action: 0.547225669338542\n",
      "Step 2: Loss 9.436394691467285 Avg Reward: 21.035809798887463 Avg Episode Len: 21.035809798887463\n",
      "Batch size: 4674 Avg action: 0.5385031539967431\n",
      "Step 3: Loss 10.236491203308105 Avg Reward: 23.110586851766865 Avg Episode Len: 23.110586851766865\n",
      "Batch size: 4358 Avg action: 0.5308460870968873\n",
      "Step 4: Loss 10.814528465270996 Avg Reward: 24.17913465700065 Avg Episode Len: 24.17913465700065\n",
      "Batch size: 4621 Avg action: 0.5243235797630953\n",
      "Step 5: Loss 12.96762752532959 Avg Reward: 26.387987884936738 Avg Episode Len: 26.387987884936738\n",
      "Batch size: 4189 Avg action: 0.5227539700891977\n",
      "Step 6: Loss 13.924625396728516 Avg Reward: 29.52271029035013 Avg Episode Len: 29.52271029035013\n",
      "Batch size: 4684 Avg action: 0.5150677459699607\n",
      "Step 7: Loss 14.40192985534668 Avg Reward: 32.639209003530794 Avg Episode Len: 32.639209003530794\n",
      "Batch size: 5098 Avg action: 0.519270997144193\n",
      "Step 8: Loss 14.608837127685547 Avg Reward: 31.987005802517704 Avg Episode Len: 31.987005802517704\n",
      "Batch size: 5084 Avg action: 0.514404988515928\n",
      "Step 9: Loss 19.266536712646484 Avg Reward: 38.330122468807524 Avg Episode Len: 38.330122468807524\n",
      "Batch size: 4889 Avg action: 0.511658669332075\n",
      "Step 10: Loss 17.130146026611328 Avg Reward: 39.236175604154184 Avg Episode Len: 39.236175604154184\n",
      "Batch size: 5007 Avg action: 0.5093699706940295\n",
      "Step 11: Loss 17.809877395629883 Avg Reward: 40.914449079401614 Avg Episode Len: 40.914449079401614\n",
      "Batch size: 5214 Avg action: 0.506064710588402\n",
      "Step 12: Loss 18.41607093811035 Avg Reward: 44.179302092609085 Avg Episode Len: 44.179302092609085\n",
      "Batch size: 5615 Avg action: 0.5025251897983918\n",
      "Step 13: Loss 16.731042861938477 Avg Reward: 42.84972124055569 Avg Episode Len: 42.84972124055569\n",
      "Batch size: 4103 Avg action: 0.5052804727523691\n",
      "Step 14: Loss 20.22242546081543 Avg Reward: 50.32217277351048 Avg Episode Len: 50.32217277351048\n",
      "Batch size: 4817 Avg action: 0.5023116732517303\n",
      "Step 15: Loss 20.11361312866211 Avg Reward: 53.132102836176124 Avg Episode Len: 53.132102836176124\n",
      "Batch size: 5042 Avg action: 0.5018102399700026\n",
      "Step 16: Loss 20.682117462158203 Avg Reward: 55.3508108687369 Avg Episode Len: 55.3508108687369\n",
      "Batch size: 5249 Avg action: 0.5018843364524502\n",
      "Step 17: Loss 24.960233688354492 Avg Reward: 67.34726256817643 Avg Episode Len: 67.34726256817643\n",
      "Batch size: 4217 Avg action: 0.502379849531171\n",
      "Step 18: Loss 23.507091522216797 Avg Reward: 65.66901029200575 Avg Episode Len: 65.66901029200575\n",
      "Batch size: 4178 Avg action: 0.49948913020754665\n",
      "Step 19: Loss 25.36041259765625 Avg Reward: 70.05158974072418 Avg Episode Len: 70.05158974072418\n",
      "Batch size: 4474 Avg action: 0.4970981523762587\n",
      "Step 20: Loss 26.38884925842285 Avg Reward: 75.3762956053068 Avg Episode Len: 75.3762956053068\n",
      "Batch size: 4824 Avg action: 0.4991178947349106\n",
      "Step 21: Loss 28.680925369262695 Avg Reward: 80.67928332687464 Avg Episode Len: 80.67928332687464\n",
      "Batch size: 5161 Avg action: 0.5017822033646189\n",
      "Step 22: Loss 29.68204689025879 Avg Reward: 85.78396243617797 Avg Episode Len: 85.78396243617797\n",
      "Batch size: 5484 Avg action: 0.49888752887326926\n",
      "Step 23: Loss 30.054288864135742 Avg Reward: 88.06609297374024 Avg Episode Len: 88.06609297374024\n",
      "Batch size: 5636 Avg action: 0.500005489814418\n",
      "Step 24: Loss 37.462791442871094 Avg Reward: 108.92701743350108 Avg Episode Len: 108.92701743350108\n",
      "Batch size: 6955 Avg action: 0.49958678277767793\n",
      "Step 25: Loss 37.21467971801758 Avg Reward: 114.27794949706622 Avg Episode Len: 114.27794949706622\n",
      "Batch size: 7158 Avg action: 0.4908267828187985\n",
      "Step 26: Loss 41.47260665893555 Avg Reward: 131.6103983418519 Avg Episode Len: 131.6103983418519\n",
      "Batch size: 8413 Avg action: 0.4851273103884108\n",
      "Step 27: Loss 46.8082389831543 Avg Reward: 140.96875 Avg Episode Len: 140.96875\n",
      "Batch size: 4511 Avg action: 0.47946947674418605\n",
      "Step 28: Loss 59.29255294799805 Avg Reward: 178.375 Avg Episode Len: 178.375\n",
      "Batch size: 5708 Avg action: 0.491625\n",
      "Step 29: Loss 79.34793090820312 Avg Reward: 235.1875 Avg Episode Len: 235.1875\n",
      "Batch size: 7526 Avg action: 0.5004375\n",
      "Step 30: Loss 106.12664794921875 Avg Reward: 320.28125 Avg Episode Len: 320.28125\n",
      "Batch size: 10249 Avg action: 0.504\n",
      "Step 31: Loss 113.29380798339844 Avg Reward: 370.21875 Avg Episode Len: 370.21875\n",
      "Batch size: 11847 Avg action: 0.5010625\n",
      "Step 32: Loss 102.3427505493164 Avg Reward: 342.40625 Avg Episode Len: 342.40625\n",
      "Batch size: 10957 Avg action: 0.499375\n",
      "Step 33: Loss 121.26050567626953 Avg Reward: 417.9375 Avg Episode Len: 417.9375\n",
      "Batch size: 13374 Avg action: 0.5005625\n",
      "Step 34: Loss 118.37271881103516 Avg Reward: 405.46875 Avg Episode Len: 405.46875\n",
      "Batch size: 12975 Avg action: 0.49975\n",
      "Step 35: Loss 114.32605743408203 Avg Reward: 364.3125 Avg Episode Len: 364.3125\n",
      "Batch size: 11658 Avg action: 0.4999375\n",
      "Step 36: Loss 125.29302215576172 Avg Reward: 427.96875 Avg Episode Len: 427.96875\n",
      "Batch size: 13695 Avg action: 0.499375\n",
      "Step 37: Loss 127.39958953857422 Avg Reward: 438.40625 Avg Episode Len: 438.40625\n",
      "Batch size: 14029 Avg action: 0.5000625\n",
      "Step 38: Loss 123.01097106933594 Avg Reward: 424.15625 Avg Episode Len: 424.15625\n",
      "Batch size: 13573 Avg action: 0.5024375\n",
      "Step 39: Loss 124.16852569580078 Avg Reward: 436.40625 Avg Episode Len: 436.40625\n",
      "Batch size: 13965 Avg action: 0.50275\n",
      "Step 40: Loss 126.92135620117188 Avg Reward: 461.90625 Avg Episode Len: 461.90625\n",
      "Batch size: 14781 Avg action: 0.502\n",
      "Step 41: Loss 129.75526428222656 Avg Reward: 474.09375 Avg Episode Len: 474.09375\n",
      "Batch size: 15171 Avg action: 0.500375\n",
      "Step 42: Loss 124.26322174072266 Avg Reward: 461.625 Avg Episode Len: 461.625\n",
      "Batch size: 14772 Avg action: 0.49725\n",
      "Step 43: Loss 112.89725494384766 Avg Reward: 412.96875 Avg Episode Len: 412.96875\n",
      "Batch size: 13215 Avg action: 0.4945\n",
      "Step 44: Loss 110.95584106445312 Avg Reward: 408.5 Avg Episode Len: 408.5\n",
      "Batch size: 13072 Avg action: 0.4949375\n",
      "Step 45: Loss 111.47840118408203 Avg Reward: 410.3125 Avg Episode Len: 410.3125\n",
      "Batch size: 13130 Avg action: 0.4958125\n",
      "Step 46: Loss 121.99129486083984 Avg Reward: 463.375 Avg Episode Len: 463.375\n",
      "Batch size: 14828 Avg action: 0.497\n",
      "Step 47: Loss 128.35113525390625 Avg Reward: 497.34375 Avg Episode Len: 497.34375\n",
      "Batch size: 15915 Avg action: 0.4985625\n",
      "Step 48: Loss 125.30651092529297 Avg Reward: 492.4375 Avg Episode Len: 492.4375\n",
      "Batch size: 15758 Avg action: 0.4996875\n",
      "Step 49: Loss 126.3235092163086 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.5004375\n",
      "Step 50: Loss 126.21560668945312 Avg Reward: 496.4375 Avg Episode Len: 496.4375\n",
      "Batch size: 15886 Avg action: 0.5004375\n",
      "Step 51: Loss 123.19182586669922 Avg Reward: 493.71875 Avg Episode Len: 493.71875\n",
      "Batch size: 15799 Avg action: 0.502\n",
      "Step 52: Loss 122.38790130615234 Avg Reward: 493.875 Avg Episode Len: 493.875\n",
      "Batch size: 15804 Avg action: 0.50175\n",
      "Step 53: Loss 123.46800231933594 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4999375\n",
      "Step 54: Loss 121.89689636230469 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4994375\n",
      "Step 55: Loss 121.34498596191406 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.50025\n",
      "Step 56: Loss 120.95594024658203 Avg Reward: 497.96875 Avg Episode Len: 497.96875\n",
      "Batch size: 15935 Avg action: 0.4993125\n",
      "Step 57: Loss 119.6577377319336 Avg Reward: 493.15625 Avg Episode Len: 493.15625\n",
      "Batch size: 15781 Avg action: 0.4981875\n",
      "Step 58: Loss 121.03897857666016 Avg Reward: 495.875 Avg Episode Len: 495.875\n",
      "Batch size: 15868 Avg action: 0.4981875\n",
      "Step 59: Loss 121.18265533447266 Avg Reward: 497.71875 Avg Episode Len: 497.71875\n",
      "Batch size: 15927 Avg action: 0.4990625\n",
      "Step 60: Loss 120.0384292602539 Avg Reward: 497.9375 Avg Episode Len: 497.9375\n",
      "Batch size: 15934 Avg action: 0.4995625\n",
      "Step 61: Loss 117.72669219970703 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.49975\n",
      "Step 62: Loss 118.59648895263672 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4999375\n",
      "Step 63: Loss 118.24256134033203 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.499875\n",
      "Step 64: Loss 115.25440979003906 Avg Reward: 494.34375 Avg Episode Len: 494.34375\n",
      "Batch size: 15819 Avg action: 0.5000625\n",
      "Step 65: Loss 117.28766632080078 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4999375\n",
      "Step 66: Loss 116.23242950439453 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.499375\n",
      "Step 67: Loss 115.12451934814453 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4996875\n",
      "Step 68: Loss 115.82317352294922 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.499625\n",
      "Step 69: Loss 115.0807113647461 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4998125\n",
      "Step 70: Loss 114.58584594726562 Avg Reward: 497.65625 Avg Episode Len: 497.65625\n",
      "Batch size: 15925 Avg action: 0.4996875\n",
      "Step 71: Loss 116.20484161376953 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.49975\n",
      "Step 72: Loss 116.2414779663086 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.5000625\n",
      "Step 73: Loss 114.40937042236328 Avg Reward: 496.59375 Avg Episode Len: 496.59375\n",
      "Batch size: 15891 Avg action: 0.4996875\n",
      "Step 74: Loss 115.80561828613281 Avg Reward: 494.65625 Avg Episode Len: 494.65625\n",
      "Batch size: 15829 Avg action: 0.49975\n",
      "Step 75: Loss 115.81230163574219 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4995625\n",
      "Step 76: Loss 114.2120590209961 Avg Reward: 497.40625 Avg Episode Len: 497.40625\n",
      "Batch size: 15917 Avg action: 0.4995\n",
      "Step 77: Loss 115.4378433227539 Avg Reward: 498.71875 Avg Episode Len: 498.71875\n",
      "Batch size: 15959 Avg action: 0.4995\n",
      "Step 78: Loss 114.73497009277344 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.50025\n",
      "Step 79: Loss 115.47477722167969 Avg Reward: 498.3125 Avg Episode Len: 498.3125\n",
      "Batch size: 15946 Avg action: 0.499375\n",
      "Step 80: Loss 112.69242858886719 Avg Reward: 483.625 Avg Episode Len: 483.625\n",
      "Batch size: 15476 Avg action: 0.499125\n",
      "Step 81: Loss 112.17866516113281 Avg Reward: 491.28125 Avg Episode Len: 491.28125\n",
      "Batch size: 15721 Avg action: 0.499375\n",
      "Step 82: Loss 112.59937286376953 Avg Reward: 494.96875 Avg Episode Len: 494.96875\n",
      "Batch size: 15839 Avg action: 0.49925\n",
      "Step 83: Loss 115.24269104003906 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4999375\n",
      "Step 84: Loss 112.85873413085938 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.499625\n",
      "Step 85: Loss 112.96265411376953 Avg Reward: 484.28125 Avg Episode Len: 484.28125\n",
      "Batch size: 15497 Avg action: 0.5\n",
      "Step 86: Loss 112.73710632324219 Avg Reward: 492.8125 Avg Episode Len: 492.8125\n",
      "Batch size: 15770 Avg action: 0.4994375\n",
      "Step 87: Loss 113.21936798095703 Avg Reward: 480.84375 Avg Episode Len: 480.84375\n",
      "Batch size: 15387 Avg action: 0.5004375\n",
      "Step 88: Loss 113.8830795288086 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4998125\n",
      "Step 89: Loss 113.23090362548828 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.500375\n",
      "Step 90: Loss 109.9495849609375 Avg Reward: 487.09375 Avg Episode Len: 487.09375\n",
      "Batch size: 15587 Avg action: 0.49975\n",
      "Step 91: Loss 106.99386596679688 Avg Reward: 456.09375 Avg Episode Len: 456.09375\n",
      "Batch size: 14595 Avg action: 0.5009375\n",
      "Step 92: Loss 109.23302459716797 Avg Reward: 488.21875 Avg Episode Len: 488.21875\n",
      "Batch size: 15623 Avg action: 0.5015\n",
      "Step 93: Loss 107.19219207763672 Avg Reward: 460.46875 Avg Episode Len: 460.46875\n",
      "Batch size: 14735 Avg action: 0.49975\n",
      "Step 94: Loss 110.65760040283203 Avg Reward: 492.3125 Avg Episode Len: 492.3125\n",
      "Batch size: 15754 Avg action: 0.5\n",
      "Step 95: Loss 108.13275146484375 Avg Reward: 471.65625 Avg Episode Len: 471.65625\n",
      "Batch size: 15093 Avg action: 0.50025\n",
      "Step 96: Loss 108.17047119140625 Avg Reward: 489.03125 Avg Episode Len: 489.03125\n",
      "Batch size: 15649 Avg action: 0.49975\n",
      "Step 97: Loss 110.97724151611328 Avg Reward: 492.5625 Avg Episode Len: 492.5625\n",
      "Batch size: 15762 Avg action: 0.5004375\n",
      "Step 98: Loss 111.04780578613281 Avg Reward: 482.0 Avg Episode Len: 482.0\n",
      "Batch size: 15424 Avg action: 0.5005\n",
      "Step 99: Loss 109.384521484375 Avg Reward: 457.34375 Avg Episode Len: 457.34375\n",
      "Batch size: 14635 Avg action: 0.5001875\n"
     ]
    }
   ],
   "source": [
    "params = Params(\n",
    "    policy_gradient_weight=PolicyGradientWeight.REWARDS_TO_GO,\n",
    "    policy_gradient_weight_baseline=PolicyGradientWeightBaseline.NONE,\n",
    ")\n",
    "# Vectorize the enviroment such that we can sample multiple trajectories concurrently\n",
    "envs = gym.make_vec(\"CartPole-v1\", num_envs=32, vectorization_mode='vector_entry_point')\n",
    "\n",
    "optim  = optax.adam(params.learning_rate)\n",
    "model = MLP(\n",
    "    in_size=4,\n",
    "    hidden_dims=params.hidden_dims + [2],\n",
    "    out_size=2,\n",
    "    key=jax.random.PRNGKey(12513)\n",
    ")\n",
    "\n",
    "model, opt_state = train(envs, model, optim, params, jax.random.PRNGKey(812342))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0d918ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss -4.427526950836182 Avg Reward: 20.148605715871255 Avg Episode Len: 20.148605715871255\n",
      "Batch size: 4505 Avg action: 0.5653456902638425\n",
      "Step 1: Loss -5.111074447631836 Avg Reward: 22.36881723573433 Avg Episode Len: 22.36881723573433\n",
      "Batch size: 4276 Avg action: 0.5536621040797353\n",
      "Step 2: Loss -4.525051593780518 Avg Reward: 22.238398336086853 Avg Episode Len: 22.238398336086853\n",
      "Batch size: 4237 Avg action: 0.5425636746869297\n",
      "Step 3: Loss -4.53015661239624 Avg Reward: 23.206105899638338 Avg Episode Len: 23.206105899638338\n",
      "Batch size: 4424 Avg action: 0.53840505332949\n",
      "Step 4: Loss -5.857970714569092 Avg Reward: 26.782580835962143 Avg Episode Len: 26.782580835962143\n",
      "Batch size: 5072 Avg action: 0.5284229532850959\n",
      "Step 5: Loss -6.337006092071533 Avg Reward: 27.58201694139194 Avg Episode Len: 27.58201694139194\n",
      "Batch size: 4368 Avg action: 0.527062237653042\n",
      "Step 6: Loss -5.943201541900635 Avg Reward: 30.148056046125078 Avg Episode Len: 30.148056046125078\n",
      "Batch size: 4813 Avg action: 0.5201961732224781\n",
      "Step 7: Loss -6.3158745765686035 Avg Reward: 32.834046734097 Avg Episode Len: 32.834046734097\n",
      "Batch size: 4103 Avg action: 0.5179976022840787\n",
      "Step 8: Loss -6.909903526306152 Avg Reward: 31.676457047581128 Avg Episode Len: 31.676457047581128\n",
      "Batch size: 5023 Avg action: 0.5147965544499382\n",
      "Step 9: Loss -8.607901573181152 Avg Reward: 35.17327021323856 Avg Episode Len: 35.17327021323856\n",
      "Batch size: 4502 Avg action: 0.5143144592951323\n",
      "Step 10: Loss -8.860368728637695 Avg Reward: 39.828957100591715 Avg Episode Len: 39.828957100591715\n",
      "Batch size: 5070 Avg action: 0.510044246339814\n",
      "Step 11: Loss -9.746983528137207 Avg Reward: 44.7173630576968 Avg Episode Len: 44.7173630576968\n",
      "Batch size: 4281 Avg action: 0.5086112561860464\n",
      "Step 12: Loss -10.869200706481934 Avg Reward: 42.84277985756026 Avg Episode Len: 42.84277985756026\n",
      "Batch size: 5476 Avg action: 0.5071757149585535\n",
      "Step 13: Loss -10.978677749633789 Avg Reward: 43.166044172062904 Avg Episode Len: 43.166044172062904\n",
      "Batch size: 5405 Avg action: 0.5060862901829486\n",
      "Step 14: Loss -11.70506763458252 Avg Reward: 48.127737874153375 Avg Episode Len: 48.127737874153375\n",
      "Batch size: 4577 Avg action: 0.5065540978001543\n",
      "Step 15: Loss -12.56395435333252 Avg Reward: 50.99688266557645 Avg Episode Len: 50.99688266557645\n",
      "Batch size: 4892 Avg action: 0.5035644073658252\n",
      "Step 16: Loss -13.588296890258789 Avg Reward: 53.142126814437034 Avg Episode Len: 53.142126814437034\n",
      "Batch size: 5098 Avg action: 0.5044203875397142\n",
      "Step 17: Loss -12.83367919921875 Avg Reward: 53.04939039408867 Avg Episode Len: 53.04939039408867\n",
      "Batch size: 5075 Avg action: 0.5029040779795323\n",
      "Step 18: Loss -14.694161415100098 Avg Reward: 61.84644260010137 Avg Episode Len: 61.84644260010137\n",
      "Batch size: 5919 Avg action: 0.5043470648423477\n",
      "Step 19: Loss -16.91419219970703 Avg Reward: 67.3752319109462 Avg Episode Len: 67.3752319109462\n",
      "Batch size: 4312 Avg action: 0.5067713938462689\n",
      "Step 20: Loss -16.246341705322266 Avg Reward: 70.5055513972056 Avg Episode Len: 70.5055513972056\n",
      "Batch size: 4509 Avg action: 0.5032758013384994\n",
      "Step 21: Loss -19.86579132080078 Avg Reward: 78.01564693202326 Avg Episode Len: 78.01564693202326\n",
      "Batch size: 4987 Avg action: 0.5014479561871905\n",
      "Step 22: Loss -20.163183212280273 Avg Reward: 86.94336113109154 Avg Episode Len: 86.94336113109154\n",
      "Batch size: 5561 Avg action: 0.5006238899923019\n",
      "Step 23: Loss -19.090946197509766 Avg Reward: 79.44502189529621 Avg Episode Len: 79.44502189529621\n",
      "Batch size: 5081 Avg action: 0.4977882940257181\n",
      "Step 24: Loss -20.415544509887695 Avg Reward: 90.2825931542461 Avg Episode Len: 90.2825931542461\n",
      "Batch size: 5770 Avg action: 0.5038272402604365\n",
      "Step 25: Loss -21.335865020751953 Avg Reward: 97.02236178369652 Avg Episode Len: 97.02236178369652\n",
      "Batch size: 6195 Avg action: 0.5075503981439168\n",
      "Step 26: Loss -23.714122772216797 Avg Reward: 102.94890054256457 Avg Episode Len: 102.94890054256457\n",
      "Batch size: 6543 Avg action: 0.5130223520591122\n",
      "Step 27: Loss -30.170949935913086 Avg Reward: 123.38539277522936 Avg Episode Len: 123.38539277522936\n",
      "Batch size: 7848 Avg action: 0.5046956848759478\n",
      "Step 28: Loss -32.751487731933594 Avg Reward: 143.8125 Avg Episode Len: 143.8125\n",
      "Batch size: 4602 Avg action: 0.5078730620155039\n",
      "Step 29: Loss -50.62141418457031 Avg Reward: 203.40625 Avg Episode Len: 203.40625\n",
      "Batch size: 6509 Avg action: 0.49026639344262296\n",
      "Step 30: Loss -50.66840744018555 Avg Reward: 203.09375 Avg Episode Len: 203.09375\n",
      "Batch size: 6499 Avg action: 0.49497635933806144\n",
      "Step 31: Loss -55.079288482666016 Avg Reward: 219.03125 Avg Episode Len: 219.03125\n",
      "Batch size: 7009 Avg action: 0.5003472222222223\n",
      "Step 32: Loss -57.962921142578125 Avg Reward: 242.15625 Avg Episode Len: 242.15625\n",
      "Batch size: 7749 Avg action: 0.5048125\n",
      "Step 33: Loss -80.8016586303711 Avg Reward: 338.90625 Avg Episode Len: 338.90625\n",
      "Batch size: 10845 Avg action: 0.5015\n",
      "Step 34: Loss -77.68254852294922 Avg Reward: 329.78125 Avg Episode Len: 329.78125\n",
      "Batch size: 10553 Avg action: 0.500375\n",
      "Step 35: Loss -81.19239044189453 Avg Reward: 333.9375 Avg Episode Len: 333.9375\n",
      "Batch size: 10686 Avg action: 0.4984375\n",
      "Step 36: Loss -86.33377838134766 Avg Reward: 350.78125 Avg Episode Len: 350.78125\n",
      "Batch size: 11225 Avg action: 0.4990625\n",
      "Step 37: Loss -86.75252532958984 Avg Reward: 359.1875 Avg Episode Len: 359.1875\n",
      "Batch size: 11494 Avg action: 0.4978125\n",
      "Step 38: Loss -96.56153869628906 Avg Reward: 387.90625 Avg Episode Len: 387.90625\n",
      "Batch size: 12413 Avg action: 0.49675\n",
      "Step 39: Loss -108.23824310302734 Avg Reward: 435.8125 Avg Episode Len: 435.8125\n",
      "Batch size: 13946 Avg action: 0.4993125\n",
      "Step 40: Loss -109.68864440917969 Avg Reward: 445.09375 Avg Episode Len: 445.09375\n",
      "Batch size: 14243 Avg action: 0.500125\n",
      "Step 41: Loss -110.96356201171875 Avg Reward: 456.6875 Avg Episode Len: 456.6875\n",
      "Batch size: 14614 Avg action: 0.5001875\n",
      "Step 42: Loss -117.33553314208984 Avg Reward: 474.625 Avg Episode Len: 474.625\n",
      "Batch size: 15188 Avg action: 0.4999375\n",
      "Step 43: Loss -115.13996124267578 Avg Reward: 463.46875 Avg Episode Len: 463.46875\n",
      "Batch size: 14831 Avg action: 0.49925\n",
      "Step 44: Loss -101.4399642944336 Avg Reward: 415.59375 Avg Episode Len: 415.59375\n",
      "Batch size: 13299 Avg action: 0.50075\n",
      "Step 45: Loss -105.80082702636719 Avg Reward: 419.8125 Avg Episode Len: 419.8125\n",
      "Batch size: 13434 Avg action: 0.5016875\n",
      "Step 46: Loss -103.72114562988281 Avg Reward: 412.625 Avg Episode Len: 412.625\n",
      "Batch size: 13204 Avg action: 0.5025\n",
      "Step 47: Loss -69.99565124511719 Avg Reward: 276.5625 Avg Episode Len: 276.5625\n",
      "Batch size: 8850 Avg action: 0.507879849137931\n",
      "Step 48: Loss -70.13958740234375 Avg Reward: 285.21875 Avg Episode Len: 285.21875\n",
      "Batch size: 9127 Avg action: 0.508\n",
      "Step 49: Loss -109.536865234375 Avg Reward: 418.8125 Avg Episode Len: 418.8125\n",
      "Batch size: 13402 Avg action: 0.504125\n",
      "Step 50: Loss -118.5765151977539 Avg Reward: 456.5 Avg Episode Len: 456.5\n",
      "Batch size: 14608 Avg action: 0.5015625\n",
      "Step 51: Loss -125.46153259277344 Avg Reward: 485.71875 Avg Episode Len: 485.71875\n",
      "Batch size: 15543 Avg action: 0.50075\n",
      "Step 52: Loss -122.52458953857422 Avg Reward: 483.21875 Avg Episode Len: 483.21875\n",
      "Batch size: 15463 Avg action: 0.5004375\n",
      "Step 53: Loss -116.04167938232422 Avg Reward: 466.46875 Avg Episode Len: 466.46875\n",
      "Batch size: 14927 Avg action: 0.500375\n",
      "Step 54: Loss -112.20429992675781 Avg Reward: 445.15625 Avg Episode Len: 445.15625\n",
      "Batch size: 14245 Avg action: 0.501\n",
      "Step 55: Loss -115.38990783691406 Avg Reward: 443.625 Avg Episode Len: 443.625\n",
      "Batch size: 14196 Avg action: 0.50375\n",
      "Step 56: Loss -119.85282897949219 Avg Reward: 464.28125 Avg Episode Len: 464.28125\n",
      "Batch size: 14857 Avg action: 0.5021875\n",
      "Step 57: Loss -114.17893981933594 Avg Reward: 439.4375 Avg Episode Len: 439.4375\n",
      "Batch size: 14062 Avg action: 0.5035625\n",
      "Step 58: Loss -111.28034973144531 Avg Reward: 437.0625 Avg Episode Len: 437.0625\n",
      "Batch size: 13986 Avg action: 0.5031875\n",
      "Step 59: Loss -125.65347290039062 Avg Reward: 491.28125 Avg Episode Len: 491.28125\n",
      "Batch size: 15721 Avg action: 0.50075\n",
      "Step 60: Loss -126.29743957519531 Avg Reward: 495.03125 Avg Episode Len: 495.03125\n",
      "Batch size: 15841 Avg action: 0.500875\n",
      "Step 61: Loss -119.59362030029297 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.500125\n",
      "Step 62: Loss -117.44217681884766 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.50075\n",
      "Step 63: Loss -115.14163208007812 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.5\n",
      "Step 64: Loss -115.38782501220703 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.499875\n",
      "Step 65: Loss -115.5823974609375 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4998125\n",
      "Step 66: Loss -116.01986694335938 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4996875\n",
      "Step 67: Loss -116.94306182861328 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.499625\n",
      "Step 68: Loss -117.39104461669922 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4995\n",
      "Step 69: Loss -116.4773941040039 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.49975\n",
      "Step 70: Loss -117.93492126464844 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.499375\n",
      "Step 71: Loss -115.9615478515625 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4995625\n",
      "Step 72: Loss -114.99876403808594 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4996875\n",
      "Step 73: Loss -115.25403594970703 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.499625\n",
      "Step 74: Loss -115.79556274414062 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4993125\n",
      "Step 75: Loss -112.30816650390625 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4994375\n",
      "Step 76: Loss -111.8311767578125 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4996875\n",
      "Step 77: Loss -112.88551330566406 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4993125\n",
      "Step 78: Loss -114.26465606689453 Avg Reward: 497.09375 Avg Episode Len: 497.09375\n",
      "Batch size: 15907 Avg action: 0.4999375\n",
      "Step 79: Loss -115.19117736816406 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.49875\n",
      "Step 80: Loss -104.69440460205078 Avg Reward: 466.875 Avg Episode Len: 466.875\n",
      "Batch size: 14940 Avg action: 0.499625\n",
      "Step 81: Loss -114.06645965576172 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.49925\n",
      "Step 82: Loss -114.1307144165039 Avg Reward: 496.625 Avg Episode Len: 496.625\n",
      "Batch size: 15892 Avg action: 0.4993125\n",
      "Step 83: Loss -111.35466766357422 Avg Reward: 488.4375 Avg Episode Len: 488.4375\n",
      "Batch size: 15630 Avg action: 0.4993125\n",
      "Step 84: Loss -113.7348861694336 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4996875\n",
      "Step 85: Loss -110.52243041992188 Avg Reward: 491.5 Avg Episode Len: 491.5\n",
      "Batch size: 15728 Avg action: 0.4995625\n",
      "Step 86: Loss -107.9793701171875 Avg Reward: 478.34375 Avg Episode Len: 478.34375\n",
      "Batch size: 15307 Avg action: 0.4995\n",
      "Step 87: Loss -115.3138656616211 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4994375\n",
      "Step 88: Loss -109.1796875 Avg Reward: 481.75 Avg Episode Len: 481.75\n",
      "Batch size: 15416 Avg action: 0.4993125\n",
      "Step 89: Loss -114.30786895751953 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4999375\n",
      "Step 90: Loss -112.76643371582031 Avg Reward: 492.5 Avg Episode Len: 492.5\n",
      "Batch size: 15760 Avg action: 0.500125\n",
      "Step 91: Loss -111.90792846679688 Avg Reward: 483.15625 Avg Episode Len: 483.15625\n",
      "Batch size: 15461 Avg action: 0.500375\n",
      "Step 92: Loss -117.2856216430664 Avg Reward: 498.03125 Avg Episode Len: 498.03125\n",
      "Batch size: 15937 Avg action: 0.49975\n",
      "Step 93: Loss -116.82843017578125 Avg Reward: 497.84375 Avg Episode Len: 497.84375\n",
      "Batch size: 15931 Avg action: 0.499\n",
      "Step 94: Loss -118.10258483886719 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.49975\n",
      "Step 95: Loss -119.86125946044922 Avg Reward: 494.75 Avg Episode Len: 494.75\n",
      "Batch size: 15832 Avg action: 0.49975\n",
      "Step 96: Loss -119.46484375 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.5000625\n",
      "Step 97: Loss -122.61502838134766 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4995625\n",
      "Step 98: Loss -122.93748474121094 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.5005\n",
      "Step 99: Loss -122.13428497314453 Avg Reward: 499.0 Avg Episode Len: 499.0\n",
      "Batch size: 15968 Avg action: 0.4996875\n"
     ]
    }
   ],
   "source": [
    "params = Params(\n",
    "    policy_gradient_weight=PolicyGradientWeight.REWARDS_TO_GO,\n",
    "    policy_gradient_weight_baseline=PolicyGradientWeightBaseline.AVERAGE_TRAJECTORY_REWARDS,\n",
    ")\n",
    "# Vectorize the enviroment such that we can sample multiple trajectories concurrently\n",
    "envs = gym.make_vec(\"CartPole-v1\", num_envs=32, vectorization_mode='vector_entry_point')\n",
    "\n",
    "optim  = optax.adam(params.learning_rate)\n",
    "model = MLP(\n",
    "    in_size=4,\n",
    "    hidden_dims=params.hidden_dims + [2],\n",
    "    out_size=2,\n",
    "    key=jax.random.PRNGKey(12513)\n",
    ")\n",
    "\n",
    "model, opt_state = train(envs, model, optim, params, jax.random.PRNGKey(812342))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db2044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6748e5e-6f3e-41fb-b66c-9eaf29adb522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rl-video-episode-0.mp4',\n",
       " 'rl-video-episode-1.mp4',\n",
       " 'rl-video-episode-10.mp4',\n",
       " 'rl-video-episode-11.mp4',\n",
       " 'rl-video-episode-12.mp4',\n",
       " 'rl-video-episode-13.mp4',\n",
       " 'rl-video-episode-14.mp4',\n",
       " 'rl-video-episode-2.mp4',\n",
       " 'rl-video-episode-3.mp4',\n",
       " 'rl-video-episode-4.mp4',\n",
       " 'rl-video-episode-5.mp4',\n",
       " 'rl-video-episode-6.mp4',\n",
       " 'rl-video-episode-7.mp4',\n",
       " 'rl-video-episode-8.mp4',\n",
       " 'rl-video-episode-9.mp4']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(os.listdir(\"./save_videos2\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
